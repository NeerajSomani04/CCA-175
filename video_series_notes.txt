Notes from videos:

From Sqoop command list and eval, video --

1) ps -ef| grep -i manager --> this command will tell us resourceManager is up and running or not.

Hadoop Certification -- sqoop import, video --
1) hostname -f --> this will give hostname for any services that is running on it.

Need to learn sqoop boundary variable related concepts of sqoop
Need to understand --compress --compress-codec option of sqoop
Need to understand Hive commands

under the hive terminal you can write below command to understand the size of file
1) dfs -du -s -h /user/hive/warehouse/order_items --> this will result combine file size under this directory

Hadoop Certification -- CCA - Spark Introduction, video --

Very important command --
Below command is used to create a soft link in spark to connect to hive from spark. This needs to be done to use HiveContext fro spark.
1) sudo ln -s /etc/hive/conf/hive-site.xml /etc/spark/conf/hive-site.xml

BigData Certification Workshop 07 - Getting started with Spark, video --

Important command for mysql connector to install on environment --->
./pip install mysql-connector-python-rf

Need to Understand fragmentation and defragmentation for mac/windows OS?? Contiguous???

hdfs fsdk "hdfs_file_path" -files -blocks --> this command will tell us how the file got distributed into different blocks.
hdfs has some default blocksize, like 64 MB or 128 MB. It can be configured also.

Below files usually located at /etc/hadoop/conf/ location:
core-site.xml file --> will have global properties
hdfs-site.xml file --> datanode, namenode, blocksize, etc details in this file.

replication factor and blocksize is 2 important things to understand from HDFS perspective.






