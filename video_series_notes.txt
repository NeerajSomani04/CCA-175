Notes from videos:

From Sqoop command list and eval, video --

1) ps -ef| grep -i manager --> this command will tell us resourceManager is up and running or not.

Hadoop Certification -- sqoop import, video --
1) hostname -f --> this will give hostname for any services that is running on it.

Need to learn sqoop boundary variable related concepts of sqoop
Need to understand --compress --compress-codec option of sqoop
Need to understand Hive commands

under the hive terminal you can write below command to understand the size of file
1) dfs -du -s -h /user/hive/warehouse/order_items --> this will result combine file size under this directory

Hadoop Certification -- CCA - Spark Introduction, video --

Very important command --
Below command is used to create a soft link in spark to connect to hive from spark. This needs to be done to use HiveContext fro spark.
1) sudo ln -s /etc/hive/conf/hive-site.xml /etc/spark/conf/hive-site.xml

BigData Certification Workshop 07 - Getting started with Spark, video --

Important command for mysql connector to install on environment --->
./pip install mysql-connector-python-rf

Need to Understand fragmentation and defragmentation for mac/windows OS?? Contiguous???

BigData Certification Workshop 08 - Getting started with Spark, video --

hdfs fsdk "hdfs_file_path" -files -blocks --> this command will tell us how the file got distributed into different blocks.
hdfs has some default blocksize, like 64 MB or 128 MB. It can be configured also.

Below files usually located at /etc/hadoop/conf/ location:
core-site.xml file --> will have global properties
hdfs-site.xml file --> datanode, namenode, blocksize, etc details in this file.

replication factor and blocksize is 2 important things to understand from HDFS perspective.
 
hdfs important blocks:
DataNode --> (gateway nodes) software process to keep track of blocks at all time, incase of failure and all. So basically keeping data.
NameNode --> keeps track of blocks associated with filename and location of blocks. MetaData of the files. Its a in-memory structure.
      It know relationship between blocks, filename and location.
SecondaryNameNode --> helper to NameNode.
WorkerNode --> perform the job of storing the data and running computations. Based on configuaration it will have dataNode, or any other          processes, that can help perform the computation.
GatewayNode submit the job to workerNode. this is to perfom the needed task.

YARN --> Execution framework (Yet Anaother Resource nagotiator) -- Basically Resource Management.
      Hadoop, Spark, etc can take care of processing the data

few important yarn commands -->
$ yarn application --list --> this will tell all the list of jobs running on cluster
$ yarn application -help --> give list of all the commands
$ yarn application -kill --> kill any specific application using applicationID


BigData Certification Workshop 09 - Getting started with Spark, video --

Building Blocks of Spark -->
SparkContext tells how application should be run (based on default or SparkConf setMaster)
yarn-client --> conf = SparkConf.setAppName("Spark Demo").setMaster("yarn-client")

pyspark and python framework difference ---> 
python is single threaded framework vs pyspark is multithread and distributed framework. This is because of sprak framework API functionality.

RDD --> Resilient Distributed DataSet
1) It is an extension of python list collection. Major difference is it stores in-memory and distributed across cluster.
2) Indexing is not supported in RDD.

DAG --> Directed Acyclic Graph 

RDD Persistance --> important concept




