Notes from videos:

From Sqoop command list and eval, video --

1) ps -ef| grep -i manager --> this command will tell us resourceManager is up and running or not.

Hadoop Certification -- sqoop import, video --
1) hostname -f --> this will give hostname for any services that is running on it.

Need to learn sqoop boundary variable related concepts of sqoop
Need to understand --compress --compress-codec option of sqoop
Need to understand Hive commands

under the hive terminal you can write below command to understand the size of file
1) dfs -du -s -h /user/hive/warehouse/order_items --> this will result combine file size under this directory

Hadoop Certification -- CCA - Spark Introduction, video --

Very important command --
Below command is used to create a soft link in spark to connect to hive from spark. This needs to be done to use HiveContext fro spark.
1) sudo ln -s /etc/hive/conf/hive-site.xml /etc/spark/conf/hive-site.xml

BigData Certification Workshop 07 - Getting started with Spark, video --

Important command for mysql connector to install on environment --->
./pip install mysql-connector-python-rf

Need to Understand fragmentation and defragmentation for mac/windows OS?? Contiguous???

BigData Certification Workshop 08 - Getting started with Spark, video --

hdfs fsdk "hdfs_file_path" -files -blocks --> this command will tell us how the file got distributed into different blocks.
hdfs has some default blocksize, like 64 MB or 128 MB. It can be configured also.

Below files usually located at /etc/hadoop/conf/ location:
core-site.xml file --> will have global properties
hdfs-site.xml file --> datanode, namenode, blocksize, etc details in this file.

replication factor and blocksize is 2 important things to understand from HDFS perspective.
 
hdfs important blocks:
DataNode --> (gateway nodes) software process to keep track of blocks at all time, incase of failure and all. So basically keeping data.
NameNode --> keeps track of blocks associated with filename and location of blocks. MetaData of the files. Its a in-memory structure.
      It know relationship between blocks, filename and location.
SecondaryNameNode --> helper to NameNode.
WorkerNode --> perform the job of storing the data and running computations. Based on configuaration it will have dataNode, or any other          processes, that can help perform the computation.
GatewayNode submit the job to workerNode. this is to perfom the needed task.

YARN --> Execution framework (Yet Anaother Resource nagotiator) -- Basically Resource Management.
      Hadoop, Spark, etc can take care of processing the data

few important yarn commands -->
$ yarn application --list --> this will tell all the list of jobs running on cluster
$ yarn application -help --> give list of all the commands
$ yarn application -kill --> kill any specific application using applicationID


BigData Certification Workshop 09 - Getting started with Spark, video --

Building Blocks of Spark -->
SparkContext tells how application should be run (based on default or SparkConf setMaster)
yarn-client --> conf = SparkConf.setAppName("Spark Demo").setMaster("yarn-client")

pyspark and python framework difference ---> 
python is single threaded framework vs pyspark is multithread and distributed framework. This is because of sprak framework API functionality.

RDD --> Resilient Distributed DataSet
1) It is an extension of python list collection. Major difference is it stores in-memory and distributed across cluster.
2) Indexing is not supported in RDD.

DAG --> Directed Acyclic Graph 

RDD Persistance --> important concept

BigData Certification Workshop 11 - Getting started with Spark, video --

Difference between Transformation and action:
    Transformation --> does lazy evaluation and generate DAG. 
    Action --> execute the job. print results, save results, etc.

How to debug and understand DAG details :
    RDD_Object_Name.toDebugString() ---> this command will tell you DAG flow.
    Example:- OrderItems.toDebugString().
    
How to read data from RDD:- in pyspark

OrderItems.first() --> this will return first element of RDD
OrderItems.take(10) --> this will return first 10 elements
OrderItems.Collect() --> Convert RDD to Python list collection
Orders.Paralalliz() --> convert python list or any collection to RDD

Need to be very carefull with the usage of collection --> Because RDD is distributed/multithreaded in-memory collection. When you convert RDD to python list using Collect() function, it will convert RDD into single threaded in-memory collection. This will cause memory issues.

When to use Collect() and when not to use -->
 1) Never use collect() function to read the data of RDD.
 
 2) You can only use Collect() API when you want to apply some function on data and that function is not available for RDD. For example, you have to use some punction from Pandas, Numpy, etc... then only convert RDD into python linear collection (list, array, dict..etc..)
 
 Practice excercise for flatMap and ReduceByKey() function:
  1) Revenue for each OrderId
  2) Get order Items generating max quatity per id. (Get each order items for max quantity)
  
  
BigData Certification Workshop 12 - Getting started with Spark, video --

reduceByKey
aggregateByKey
groupByKey

BigData Certification Workshop 13 - Getting started with Spark, video --

problem statement:- 
1) Get revenue as well as count of items for each order_id
2) For each order_id get the revenue and max subtotal for that order_id (2, (549.98, 250.0))



