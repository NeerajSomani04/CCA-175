Below are the points one should remember Exam Day:

a) How to check mysql connection details, if not given directly. 
   for this best is to run the sqoop command in Cloudera Hadoop Cluster VM.
Note: I have seen on cloudera test video that cluster node details was given directly and we need to use it as is.

b) Some concepts to understand:
      1) If source or input is in parquet, json, avro, orc, csv format. Then we need to read data by sqlContext API and create DF.
            then create a tempTable from DF and apply needed Hive operations.
      concerns/questions: Need to understand how to apply rdd similar operations (like --> filter, flatMap, map, reduceByKey, groupByKey, etc here in hive). Also, how to save data in a specific required file format and output format. 
      Current understanding/Answer: 
            once you create data frame, you can apply most of the rdd related operattion, below is explanation:
            1) filter --> output will be DataFrame
            2) map, flatMap --> output will be RDD
            3) reduceByKey, AggregateByKey, etc --> once RDD created, we can apply all RDD operations
            4) write/save --> once we have our final RDD, we can create needed DF and save data as per need. 
      
c) Open 4 applications on CDH:
    1) Sublim / any other text editor
    2) Cloudera Manager
    3) Terminal 
        a) for Hive
        b) spark-shell --> 2 terminal to work on 2 problems simultaneously. This is because code usually takes long time to run.
        c) for mysql
        d) sqoop
        e) need to update from here if needed for flume/impala/
        
d) There should be some UI in cloudera that provides information about Namenode. NameNode will give us information about many important components.

e) understand memory and cluster capacity will help to leverage cluster properly and run jobs in proper manner. This way we can execute jobs fast and save some time in exam.

f) At time of certification, cloudera wouldn't allow us to connect to mysql. we need to validate any mysql queries through sqoop.

g) to get details about cluster, which is in Yarn mode.
      cat /etc/hadoop/conf/yarn-site.xml
      linux command --> once you open the file type "/yarn.resourcemanager" and hit enter. this command will serach matching pattern in file.
      yarn.resourcemanager.webapp.address --> this is the property name that will give us information about yarn resource manager address.

h) Durga sir mentioned that their will be different datasets and not only one like retail_db. Although, questions will be much simpler than what we are practicing in video series. 3 to 8 lines of code in 10 min for 1 question is the expectation. but don't completely rely on this statement.

i) dont use tabs for typing in spark or sublime text for exam. Spark treats tab differently and can create some issues in exam. Instead use space twice, which is equal to one tab. (one tab == two space bars)

j) some important questions:
      1) How to sort String data into Ascending and Descending order
      2) How to read multiple data files from local file system into spark and create RDD from it.
      3) 

k) List details of all configuartion files related to each technology. This will help us find any configuartion related details at the time of certification:

      1) Hadoop conf file --> cat /etc/hadoop/conf/core-site.xml, cat /etc/hadoop/conf/hdfs-site.xml
      2) yarn conf file --> cat /opt/yarn/conf/yarn-site.xml
      3) spark conf file -->
      under local cluster home directory (/home/dkothari). (need to check local and hdfs dir for cloudera)
               $ view /etc/spark/conf/spark-env.sh
               $ view /etc/spark/conf/spark-defaults.conf
      4) mysql db conf related details --> like jdbc url connection url, etc..
      5) Hive warehouse details on cluster -->
      6) sqoop conf file --> details for compression codec --> 
            --> cat /etc/hadoop/conf/core-site.xml
            --> /codec
l) 


