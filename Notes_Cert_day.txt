Below are the points one should remember Exam Day:

a) How to check mysql connection details, if not given directly. 
   for this best is to run the sqoop command in Cloudera Hadoop Cluster VM.
Note: I have seen on cloudera test video that cluster node details was given directly and we need to use it as is.

b) Some concepts to understand:
      1) If source or input is in parquet, json, avro, orc, csv format. Then we need to read data by sqlContext API and create DF.
            then create a tempTable from DF and apply needed Hive operations.
      concerns/questions: Need to understand how to apply rdd similar operations (like --> filter, flatMap, map, reduceByKey, groupByKey, etc here in hive). Also, how to save data in a specific required file format and output format. 
      Current understanding/Answer: 
            once you create data frame, you can apply most of the rdd related operattion, below is explanation:
            1) filter --> output will be DataFrame
            2) map, flatMap --> output will be RDD
            3) reduceByKey, AggregateByKey, etc --> once RDD created, we can apply all RDD operations
            4) write/save --> once we have our final RDD, we can create needed DF and save data as per need. 
      
c) Open 4 applications on CDH:
    1) Sublim / any other text editor
    2) Cloudera Manager
    3) Terminal 
        a) for Hive
        b) spark-shell --> 2 terminal to work on 2 problems simultaneously. This is because code usually takes long time to run.
        c) for mysql
        d) sqoop
        e) need to update from here if needed for flume/impala/
        
d) There should be some UI in cloudera that provides information about Namenode. NameNode will give us information about many important components.

e) understand memory and cluster capacity will help to leverage cluster properly and run jobs in proper manner. This way we can execute jobs fast and save some time in exam.

f) At time of certification, cloudera wouldn't allow us to connect to mysql. we need to validate any mysql queries through sqoop.

g) to get details about cluster, which is in Yarn mode.
      cat /etc/hadoop/conf/yarn-site.xml
      linux command --> once you open the file type "/yarn.resourcemanager" and hit enter. this command will serach matching pattern in file.
      yarn.resourcemanager.webapp.address --> this is the property name that will give us information about yarn resource manager address.

h) Durga sir mentioned that their will be different datasets and not only one like retail_db. Although, questions will be much simpler than what we are practicing in video series. 3 to 8 lines of code in 10 min for 1 question is the expectation. but don't completely rely on this statement.

i) dont use tabs for typing in spark or sublime text for exam. Spark treats tab differently and can create some issues in exam. Instead use space twice, which is equal to one tab. (one tab == two space bars)

j) some important questions:
      1) How to sort String data into Ascending and Descending order
      2) How to read multiple data files from local file system into spark and create RDD from it.
      3) 

k) List details of all configuartion files related to each technology. This will help us find any configuartion related details at the time of certification:

      1) Hadoop conf file --> cat /etc/hadoop/conf/core-site.xml, cat /etc/hadoop/conf/hdfs-site.xml
      2) yarn conf file --> cat /opt/yarn/conf/yarn-site.xml
      3) spark conf file -->
      under local cluster home directory (/home/dkothari). (need to check local and hdfs dir for cloudera)
               $ view /etc/spark/conf/spark-env.sh
               $ view /etc/spark/conf/spark-defaults.conf
      4) mysql db conf related details --> like jdbc url connection url, etc..
      5) Hive warehouse details on cluster -->
      6) sqoop conf file --> details for compression codec --> 
            --> cat /etc/hadoop/conf/core-site.xml
            --> /codec

l) File format notes:

TO READ:
for text file:
sc.textFile("/public/retail_db/orders")
or
sqlContext.read.text("/public/retail_db/orders")

for json file: (same for orc, parquet)
sqlContext.read.json("/public/retail_db_json/orders")
or 
sqlContext.load("/public/retail_db/orders", "json")

for csv file: (same for avro)
   we need to first import needed packages from databricks. There are 2 ways to do so
   1) spark-shell --master yarn --conf spark.ui.port=12654 --num-executors 2 --executor-memory 512M --packages com.databricks:spark-avro_2.10:2.0.1
   Notes:--> (for csv files, com.databricks:spark-csv_2.11:1.5.0)
   Notes:--> (sometime we can use jar files as well by using --jars 'jarfilename' argument)
   2) 
   
After you included package
sqlContext.load("/public/retail_db_csv/orders", "com.databricks.spark.csv")

TO WRITE:
for textfile
rdd.saveAsTextFile("/home/dkothari/orders", classOf["org.apache.hadoop.io.compress.SnappyCodec"])
or 
sqlContext.write.text("/home/dkothari/orders")
sc.

for orc file: (same for json, parquet)
sqlContext.setConf(“spark.sql.orc.compression.codec”,”snappy”)
DF.write.orc("/public/retail_db_orc/orders_orc", "snappy")
or 
sqlContext.save("/public/retail_db_orc/orders_orc", "orc", "snappy")

for avro file format: (same for csv)
import com.databricks.spark.avro._;
DF.save("/public/retail_db_avro/orders_avro", "com.databricks.spark.avro")
Note:--> to verify that the data gets loaded to the needed directory use below command.
sqlContext.load("/public/retail_db_avro/orders_avro", "com.databricks.spark.avro").show()



Sample Exam question format:
   1) read data from file (data could be in any format)
   2) aggregate (apply some transformation)
   3) Sort (with multiple key in ascending, descending order)
   4) save in file or table. (Hdfs or local) (also, with some specific delimiter, line terminator)
   5) apply some compression method on data file.
   6) Also, need to understand how to overwrite existing file data

Notes:-->
1) How to save json dataframe to text file.
Answer: (Here I need to see how the column name of dataframe reacts to rdd and then textfile)
1) first we need to convert dataframe to rdd 2) then apply map function to convert data into needed format
       3) the save data in needed format.

