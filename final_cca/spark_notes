1. saving below command to remind all the string function that we can implement on rdd

val cust2013Aug = orders.
						filter(order => order.split(",")(1).contains("2013-08")).
						map(rec => rec.split(",")(2).toInt)

2. Row level transformation (MAP => Get date from orders in Integer format)
orders.map(e => (e.split(",")(1).substring(0,10).replace("-", "")).toInt)
            
 // Left Outer Join // it will take all data from left but leaving data from left that are also present in right.
val ordersLeftOuterJoin = ordersMap.leftOuterJoin(orderItemsMap)
							.filter(order => order._2._2 == None)
							.map(order => order._2._1)

3. 
Steps To Initialize Spark Context Programmatically
import org.apache.spark.{SparkConf, SparkContext} val conf = new SparkConf().setAppName("Daily Revenue").setMaster("yarn-client") val sc = new SparkContext(conf)

Get all conf parameters
sc.getConf.getAll.foreach(println)

----------------- spark DF and spark SQL notes --------------

case class Orders(
order_id: Int,
order_date: String,
order_customer_id: Int,
order_status: String);

case class OrderItems(
order_item_id: Int,
order_item_order_id: Int,
order_item_product_id: Int,
order_item_quantity: Int,
order_item_subtotal: Double,
order_item_price: Double)

// Orders DF

val orders = sc.textFile("file:///home/cloudera/Desktop/CCA175 Commands/data/retail_db/orders")

val ordersDF = orders.map(rec => {
val r = rec.split(",")
Orders(r(0).toInt,r(1),r(2).toInt,r(3))
}).toDF

// Order Items DF

val order_items = sc.textFile("file:///home/cloudera/Desktop/CCA175 Commands/data/retail_db/order_items")

val orders_itemsDF = order_items.map(rec => {
val r = rec.split(",")
OrderItems(r(0).toInt,r(1).toInt,r(2).toInt,r(3).toInt,r(4).toDouble,r(5).toDouble)
}).toDF

// Quering the DF
ordersDF.registerTempTable("orders")
sqlContext.sql("select * from orders limit 10").collect().foreach(println)

// Filter in DF
val ordersFiltered = ordersDF.filter(ordersDF("order_status") === "COMPLETE" || ordersDF("order_status") === "CLOSED")

// Join in DF
val orderJoin = ordersFiltered.Join(order_itemsDF, ordersFiltered("order_id") === order_itemsDF("order_item_order_id"))

// Group By in DF
orderJoin.groupBy("order_date").
agg(sum("order_item_subtotal")).
show()

// Doing the above operation using SQL
ordersDF.registerTempTable("orders")
order_itemsDF.registerTempTable("order_items")

sqlContext.sql("SELECT order_date, SUM(order_item_subtotal) FROM orders JOIN order_items ON order_id = order_item_order_id WHERE order_status IN ('COMPLETED','CLOSED') GROUP BY order_date").show()

------------------------------

val input = sc.textFile("spark_files/emp.txt")
case class employee(id: Int, name: String, salary: Int, dept: String)
val input_split = input.map(line => line.split(","))
val emprdd = input_split.map(x => employee(x(0).toInt, x(1), x(2).toInt, x(3)))
val empDF = emprdd.toDF()

//to chnage particular column but not all
empDF.withColumnRenamed("ID", "A").show()

// to drop a column
empDF.drop("ID").show()

//to select specific columns
empDF.select(empDF("ID"),empDF("Name")).show()

// to get distinct values from a column
empDF.select(empDF("Dept")).distinct.show()

// to filter few records from a column
empDF.filter(empDF("Dept") === "HR").show()
---- another way to filter records -- 
empDF.filter($"Dept" === "HR").show()

----Inner join DF ----
empDF.join(deptDF, empDF("dept") === deptDF("did")).show()
//if you want display of full data in columns ----
empDF.join(deptDF, empDF("dept") === deptDF("did")).show(truncate=false)

---- Left Outer join -------
empDF.join(deptDF, empDF("dept") === deptDF("did"),"left_outer").show(truncate=false)
--- right Outer join -------
empDF.join(deptDF, empDF("dept") === deptDF("did"),"right_outer").show(truncate=false)
---- full outer join ----
empDF.join(deptDF, empDF("dept") === deptDF("did"),"full_outer").show(truncate=false)

//Order By --- sorting of data frame
empDF.orderBy(empDF("Id")).show()
empDF.orderBy(empDF("salary").desc).show()

---- spark sql --
empDF.registerTempTable("employee")
val mid_sal = sqlContext.sql("select name, dept, salary from employee where salary >= 2000 and salary <= 5000")

--- to read a hive table in spark --
// here rajeshk is the DB name and dept is table name
val deptDF = sqlContext.sql("select * from rajeshk.dept");

            
