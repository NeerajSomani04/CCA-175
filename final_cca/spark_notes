1. saving below command to remind all the string function that we can implement on rdd

val cust2013Aug = orders.filter(order => order.split(",")(1).contains("2013-08")).map(rec => rec.split(",")(2).toInt)

2. Row level transformation (MAP => Get date from orders in Integer format)
orders.map(e => (e.split(",")(1).substring(0,10).replace("-", "")).toInt)
            
// Left Outer Join // it will take all data from left but leaving data from left that are also present in right.
// ordersMap and orderItemsMap are RDD in (k, v) pair form.
val ordersLeftOuterJoin = ordersMap.leftOuterJoin(orderItemsMap).filter(order => order._2._2 == None).map(order => order._2._1)

---------------------- Important DF functions: ----------
to_date(from_unixtime(col("order_date")/1000)).alias("order_formatted_date")
to_date() --> will print date in the format YYYY-MM-DD
from_unixtime() --> will convert date format value from unixtime format to regular datetime format.
alias() --> is used to assign an alias name of column
------- agg(round(sum("order_item_subtotal"),2) -----
agg() --> is used to calculate aggregate values
round(colname, 2) --> is used to round decimal values to 2 decimal points
sum() --> is used to calculate the sum of all values of that column
min(), max(), avg() --> some other function works as name specifies
countDistinct() --> used to count distinct values of that column
count() --> is used to count values of that column

---- string operations on DF columns ---- also I can try similar function in SparkSQL ---
df = df.select(col("*"), substring(col("a"), 4, 6).as("c"))

+------+---+---+
|     a|  b|  c|
+------+---+---+
|foobar|foo|bar|
+------+---+---+

----------- Important spark SQL notes -------
DF.registerTempTable("temp_tblName");
to_date(from_unixtime(cast(order_date/1000 as bigint)))
cast() --> function to cast the datatype of column
cast(sum(order_item_subtotal) as DECIMAL (10,2)) 

----------------- spark DF and spark SQL notes --------------
case class Orders(
order_id: Int,
order_date: String,
order_customer_id: Int,
order_status: String);

case class OrderItems(
order_item_id: Int,
order_item_order_id: Int,
order_item_product_id: Int,
order_item_quantity: Int,
order_item_subtotal: Float,
order_item_price: Double)

// OrdersDF from orders RDD 
val orders = sc.textFile("file:///home/cloudera/Desktop/CCA175 Commands/data/retail_db/orders")

val ordersDF = orders.map(rec => {
val r = rec.split(",")
Orders(r(0).toInt,r(1),r(2).toInt,r(3))
}).toDF

// Order Items DF
val order_items = sc.textFile("file:///home/cloudera/Desktop/CCA175 Commands/data/retail_db/order_items")
val orders_itemsDF = order_items.map(rec => {
val r = rec.split(",")
OrderItems(r(0).toInt,r(1).toInt,r(2).toInt,r(3).toInt,r(4).toFloat,r(5).toDouble)
}).toDF

// Quering the DF
ordersDF.registerTempTable("orders")
sqlContext.sql("select * from orders limit 10").collect().foreach(println)

// Filter in DF
val ordersFiltered = ordersDF.filter(ordersDF("order_status") === "COMPLETE" || ordersDF("order_status") === "CLOSED")

// Join in DF
val orderJoin = ordersFiltered.Join(order_itemsDF, ordersFiltered("order_id") === order_itemsDF("order_item_order_id"))

// Group By in DF
orderJoin.groupBy("order_date").agg(sum("order_item_subtotal")).show()

// Doing the above operation using SQL
ordersDF.registerTempTable("orders")
order_itemsDF.registerTempTable("order_items")

sqlContext.sql("SELECT order_date, SUM(order_item_subtotal) FROM orders JOIN order_items ON order_id = order_item_order_id WHERE order_status IN ('COMPLETED','CLOSED') GROUP BY order_date").show()

------------------------------
//Below are some Dataframe functions

//to change particular column but not all
empDF.withColumnRenamed("ID", "A").show()

// to drop a column
empDF.drop("ID").show()

//to select specific columns
empDF.select(empDF("ID"),empDF("Name")).show()

// to get distinct values from a column
empDF.select(empDF("Dept")).distinct.show()

// to filter few records from a column
empDF.filter(empDF("Dept") === "HR").show()
---- another way to filter records -- 
empDF.filter($"Dept" === "HR").show()

----Inner join DF ----
empDF.join(deptDF, empDF("dept") === deptDF("did")).show()
//if you want display of full data in columns ----
empDF.join(deptDF, empDF("dept") === deptDF("did")).show(truncate=false)

---- Left Outer join ------- // here DF.show(truncate=false) is used to show full data values of column.
empDF.join(deptDF, empDF("dept") === deptDF("did"),"left_outer").show(truncate=false)
--- right Outer join -------
empDF.join(deptDF, empDF("dept") === deptDF("did"),"right_outer").show(truncate=false)
---- full outer join ----
empDF.join(deptDF, empDF("dept") === deptDF("did"),"full_outer").show(truncate=false)

//Order By --- sorting of data frame
empDF.orderBy(empDF("Id"), empDF("salary").desc).show()

---- spark sql ----------
empDF.registerTempTable("employee")
val mid_sal = sqlContext.sql("select name, dept, salary from employee where salary >= 2000 and salary <= 5000")

--- to read a hive table in spark --
// here rajeshk is the DB name and dept is table name
val deptDF = sqlContext.sql("select * from rajeshk.dept");

-------------------
3. 
Steps To Initialize Spark Context Programmatically
import org.apache.spark.{SparkConf, SparkContext} 
val conf = new SparkConf().setAppName("Daily Revenue").setMaster("yarn-client") 
val sc = new SparkContext(conf)

Get all conf parameters
sc.getConf.getAll.foreach(println)
