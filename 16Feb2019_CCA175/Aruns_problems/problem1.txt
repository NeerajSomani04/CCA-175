problem1

1. sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--table orders \
--target-dir /user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/orders \
--delete-target-dir \
--as-avrodatafile \
--compress \
--compression-codec 'org.apache.hadoop.io.compress.SnappyCodec'

2. sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--table order_items \
--target-dir /user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/order-items \
--delete-target-dir \
--as-avrodatafile \
--compress \
--compression-codec 'org.apache.hadoop.io.compress.SnappyCodec'

Note for above commands:- Snappy compressed avro files will not have “snappy” in their compressed file name. You need to compare the file size of compressed and uncompressed files and verify if the compression works fine.

launch spark shell using below command 

spark-shell --master yarn --conf spark.ui.port=12666 --packages com.databricks:spark-avro_2.10:2.0.1
import com.databricks.spark.avro._
sqlContext.setConf("spark.sql.avro.compression.codec","snappy")

Neeraj -- not using pyspark because it is on python 2.7 and i am not sure the impact.
pyspark --master yarn --conf spark.ui.port=12765 --packages com.databricks:spark-avro_2.10:2.0.1

3. 

val ordersDF = sqlContext.read.avro("/user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/orders")
val orderItemsDF = sqlContext.read.avro("/user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/order-items")

4. 
val joinedDF = ordersDF.join(orderItemsDF, $"order_id" === $"order_item_order_id")
val selectDF = joinedDF.withColumn("order_date", to_date(from_unixtime($"order_date"/1000))).select("order_id", "order_date", "order_status", "order_item_subtotal")

import org.apache.spark.sql.functions._ // this command is required only if it is not already imported in environment by default

val aggDF = selectDF.groupBy("order_date", "order_status").agg(countDistinct("order_id").alias("total_orders"), round(sum("order_item_subtotal"),2).alias("total_amount"))

val finalDF = aggDF.sort($"order_date".desc, $"order_status", $"total_amount".desc, $"total_orders")


