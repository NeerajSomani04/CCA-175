problem1

1. sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--table orders \
--target-dir /user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/orders \
--delete-target-dir \
--as-avrodatafile \
--compress \
--compression-codec 'org.apache.hadoop.io.compress.SnappyCodec'

2. sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--table order_items \
--target-dir /user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/order-items \
--delete-target-dir \
--as-avrodatafile \
--compress \
--compression-codec 'org.apache.hadoop.io.compress.SnappyCodec'

Note for above commands:- Snappy compressed avro files will not have “snappy” in their compressed file name. You need to compare the file size of compressed and uncompressed files and verify if the compression works fine.

launch spark shell using below command 

spark-shell --master yarn --conf spark.ui.port=12666 --packages com.databricks:spark-avro_2.10:2.0.1


import com.databricks.spark.avro._
sqlContext.setConf("spark.sql.avro.compression.codec","snappy")

Neeraj -- not using pyspark because it is on python 2.7 and i am not sure the impact.
pyspark --master yarn --conf spark.ui.port=12765 --packages com.databricks:spark-avro_2.10:2.0.1

3. 

val ordersDF = sqlContext.read.avro("/user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/orders")
val orderItemsDF = sqlContext.read.avro("/user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/order-items")

4. a
val joinedDF = ordersDF.join(orderItemsDF, $"order_id" === $"order_item_order_id")
val selectDF = joinedDF.withColumn("order_date", to_date(from_unixtime($"order_date"/1000))).select("order_id", "order_date", "order_status", "order_item_subtotal")

import org.apache.spark.sql.functions._ // this command is required only if it is not already imported in environment by default

val aggDF = selectDF.groupBy("order_date", "order_status").agg(countDistinct("order_id").alias("total_orders"), round(sum("order_item_subtotal"),2).alias("total_amount"))

val DataFrameResultDF = aggDF.sort($"order_date".desc, $"order_status", $"total_amount".desc, $"total_orders")

4. b 
ordersDF.registerTempTable("ordersTemp")
orderItemsDF.registerTempTable("OrderItemsTemp")

val SqlResultDF = sqlContext.sql("""select cast(from_unixtime(order_date/1000) as date) as order_date, order_status, count(distinct order_id) as total_orders, sum(order_item_subtotal) as total_amount from ordersTemp join OrderItemsTemp on ordersTemp.order_id = orderItemsTemp.order_item_order_id group by cast(from_unixtime(order_date/1000) as date), order_status order by order_date desc, order_status, total_amount desc, total_orders""")

4. c
Not able to perform this in RDD yet

5.
sqlContext.setConf("spark.sql.parquet.compression.codec","gzip")
DataFrameResultDF.write.parquet("/user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/result4a-gzip")
SqlResultDF.write.parquet("/user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/result4b-gzip")

6. 
sqlContext.setConf("spark.sql.parquet.compression.codec","snappy")
DataFrameResultDF.write.parquet("/user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/result4a-snappy")
SqlResultDF.write.parquet("/user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/result4b-snappy")

7. 
DataFrameResultDF.map(x => x(0) + "," + x(1) + "," + x(2) + "," + x(3)).saveAsTextFile("/user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/result4a-csv")

SqlResultDF.map(x => x(0) + "," + x(1) + "," + x(2) + "," + x(3)).saveAsTextFile("/user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/result4b-csv")

Notes -->
below is one example, how we can implement compression logic on RDD -->
sc.saveAsTextFile(path, classOf(org.apache.hadoop.io.compress.SnappyCodec"))

8.
mysql -u retail_user -h ms.itversity.com -p
password: itversity

use retail_export; // this command is used to connect to correct DB

create table dkothari_result_arunproblem1(order_date varchar(255) not null, order_status varchar(1000) not null, total_orders int, total_amount decimal(10,2), constraint pk_order_result primary key (order_date,order_status));

sqoop eval \
--connect jdbc:mysql://ms.itversity.com:3306/retail_export \
--username retail_user \
--password itversity \
--query "describe dkothari_result_arunproblem1"

sqoop export \
--connect jdbc:mysql://ms.itversity.com:3306/retail_export \
--username retail_user \
--password itversity \
--table dkothari_result_arunproblem1 \
--export-dir /user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/result4a-csv \
--input-fields-terminated-by ','

to verify the count from hdfs files -->
hdfs dfs -cat /user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/result4a-csv/* | wc -l

to verify the count from Dataframe -->
DataFrameResultDF.count


