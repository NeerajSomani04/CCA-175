problem1

1. sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--table orders \
--target-dir /user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/orders \
--delete-target-dir \
--as-avrodatafile \
--compress \
--compression-codec 'org.apache.hadoop.io.compress.SnappyCodec'

2. sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--table order_items \
--target-dir /user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/order-items \
--delete-target-dir \
--as-avrodatafile \
--compress \
--compression-codec 'org.apache.hadoop.io.compress.SnappyCodec'

Note for above commands:- Snappy compressed avro files will not have “snappy” in their compressed file name. You need to compare the file size of compressed and uncompressed files and verify if the compression works fine.

launch spark shell using below command 

spark-shell --master yarn --conf spark.ui.port=12666 --packages com.databricks:spark-avro_2.10:2.0.1


import com.databricks.spark.avro._
sqlContext.setConf("spark.sql.avro.compression.codec","snappy")

Neeraj -- not using pyspark because it is on python 2.7 and i am not sure the impact.
pyspark --master yarn --conf spark.ui.port=12765 --packages com.databricks:spark-avro_2.10:2.0.1

3. 

val ordersDF = sqlContext.read.avro("/user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/orders")
val orderItemsDF = sqlContext.read.avro("/user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/order-items")

4. a
val joinedDF = ordersDF.join(orderItemsDF, $"order_id" === $"order_item_order_id")
val selectDF = joinedDF.withColumn("order_date", to_date(from_unixtime($"order_date"/1000))).select("order_id", "order_date", "order_status", "order_item_subtotal")

import org.apache.spark.sql.functions._ // this command is required only if it is not already imported in environment by default

val aggDF = selectDF.groupBy("order_date", "order_status").agg(countDistinct("order_id").alias("total_orders"), round(sum("order_item_subtotal"),2).alias("total_amount"))

val DataFrameResultDF = aggDF.sort($"order_date".desc, $"order_status", $"total_amount".desc, $"total_orders")

4. b 
ordersDF.registerTempTable("ordersTemp")
orderItemsDF.registerTempTable("OrderItemsTemp")

val SqlResultDF = sqlContext.sql("""select cast(from_unixtime(order_date/1000) as date) as order_date, order_status, count(distinct order_id) as total_orders, sum(order_item_subtotal) as total_amount from ordersTemp join OrderItemsTemp on ordersTemp.order_id = orderItemsTemp.order_item_order_id group by cast(from_unixtime(order_date/1000) as date), order_status order by order_date desc, order_status, total_amount desc, total_orders""")

4. c
Not able to perform this in RDD yet

5.
sqlContext.setConf("spark.sql.parquet.compression.codec","gzip")
DataFrameResultDF.write.parquet("/user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/result4a-gzip")
SqlResultDF.write.parquet("/user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/result4b-gzip")

6. 
sqlContext.setConf("spark.sql.parquet.compression.codec","snappy")
DataFrameResultDF.write.parquet("/user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/result4a-snappy")
SqlResultDF.write.parquet("/user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/result4b-snappy")
