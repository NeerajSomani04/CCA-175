problem1

1. sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--table orders \
--target-dir /user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/orders \
--delete-target-dir \
--as-avrodatafile \
--compress \
--compression-codec 'org.apache.hadoop.io.compress.SnappyCodec'

2. sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--table order_items \
--target-dir /user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/order-items \
--delete-target-dir \
--as-avrodatafile \
--compress \
--compression-codec 'org.apache.hadoop.io.compress.SnappyCodec'

Note for above commands:- Snappy compressed avro files will not have “snappy” in their compressed file name. You need to compare the file size of compressed and uncompressed files and verify if the compression works fine.

launch spark shell using below command 

spark-shell --master yarn --conf spark.ui.port=12666 --packages com.databricks:spark-avro_2.10:2.0.1
import com.databricks.spark.avro._

sqlContext.setConf("spark.sql.avro.compression.codec","snappy")


3. 

val ordersDF = sqlContext.read.avro("/user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/orders")
val ordersDF = sqlContext.read.avro("/user/dkothari/16Feb2019_CCA/arun_problems/attempt1/problem1/orders")
