All the details are at Itversity.com website under specific courses
       
Video 01 - Scala Fundamentals --> Have instructions on scala setup on mac with intellij IDE
Video 02 - Basic Scala Programming --> Var, val, loops
Video 03 - Functions --> definiing functions and anaonymous funcs.
Video 04 - OOPs concepts --> Class
     below command is used to set Java_Home environment path, so that we can use :javap command of scala.
        export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk
    Few Important points for Scala Class:
       Scala Class defination by default generate default constructor.
       If there is any argumnet of class with Val or Var, then those will be only argument of class and not variable of class.
       If arguments are defined by Val (immutable), getter method will be available by constructor.
       If arguments are defined by Var (mutable), getter and setter method will be available by constructor.
Video 05 - OOPs concepts --> Objects
       Objects are singleton class, hence it can not be intitable more than once. 
       There is no constructor for Objects.
       Companion Objects:- Class name and Object name are same.
Video 06 - OOPs concepts --> Case Classes
      This basically gives you many functionality as a inbuilt, without writting much code.
Video 07 - Scala Fundamentals --> Collection
      Sequence --> hold number of elements
          Array -->
          List -->
      Set --> unique values
      Map --> (Key,value) pair
      
MySql DB Details: (at time of certification, cloudera wouldn't allow to connect to mysql. we need to validate any mysql queries through sqoop)
       users --> 
              retail_user
              hr_user
              nyse_user
       available databases -->
              retail_db
              hr_db
              nyse_db
       hostname --> ms.itversity.com
       password --> itversity
       Commands --> 
              to connect to mysql --> mysql -u retail_user -h ms.itversity.com -p itversity
              to see available databases --> show databases;
              to connect to db --> use retail_db;
              to see tables --> show tables;
       
Sqoop commands:
       how to build connection string for mysql database.
              --> --connect jdbc:mysql://ms.iteversity.com:3306/retail_db
                  where, hostname --> ms.itevrsity.com
                         port --> 3306 (this is usually default port)
                         db_name --> retails_db
       Things to remember for --split-by option of sqoop command:
              1) Column should be indexed
              2) values should be sparse / or it should be sequentially generated or evenly incremented
              3) it should not have null values
              4) we need to use this always when we use --query aurgument
              
       --boundary-query aurgument is used for creating split while importing data.
       --table aurgument should not be used with --query aurgument.
       
Need to start from below video
video 44 -- apache sqoop -- sqoop import -- delimiters and handling nulls
Example Sqoop command:--
            sqoop import \
            --connect jdbc:mysql://ms.itversity.com:3306/hr_db \
            --username hr_user \
            --password itversity \
            --table employees \
            --warehouse-dir /user/dkothari/sqoop_import/hr_db \
            --null-non-string -1 \
            --fields-terminated-by "\t" \
            --lines-terminated-by ":"
            
video 47 -- Sqoop command for Hive import --
       sqoop import \
       --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
            --username retail_user \
            --password itversity \
            --table order_items \
            --hive-import \
            --hive-database dkothari_sqoop_import \
            --hive-table order_items \
            --num-mappers 2


sqoop import \
       --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
            --username retail_user \
            --password itversity \
            --table orders \
            --hive-import \
            --hive-database dkothari_sqoop_import \
            --hive-table orders \
            --hive-overwrite \
            --num-mappers 2
            
video 50 -->
       Question : Create Hive table "Daily_Revenue". based on the data fetched from other tables.
       Command: --> 
              create table daily_revenue as
              select order_date, sum(order_item_subtotal) daily_revenue
              from orders join order_items on 
              order_id = order_item_order_id
              where order_date like '2013-07%'
              group by order_date;
video 51 --> Sqoop export -- create hive table
       mysql command for creating table in retail_export --> create table daily_revenue_dkothari
              (order_date varchar(30), revenue float);
     
       sqoop export command-->
              sqoop export \
                     --connect jdbc:mysql://ms.itversity.com:3306/retail_export \
                     --username retail_user \
                     --password itversity \
                     --table daily_revenue_dkothari \
                     --export-dir /apps/hive/warehouse/dkothari_sqoop_import.db/daily_revenue \
                     --input-fields-terminated-by "\001"  
video 53 --> 
       mysql demo table --> 
              create table daily_revenue_dkothari_demo
              (revenue float, order_date varchar(30), description varchar(200));
     
      sqoop export command -->
              --column aurgument is very important. The order of column names in column aurgument should be same as source table                                 structure (which hdfs directory or hive table). Below is example:
              
              sqoop export \
                     --connect jdbc:mysql://ms.itversity.com:3306/retail_export \
                     --username retail_user \
                     --password itversity \
                     --table daily_revenue_dkothari_demo \
                     --export-dir /apps/hive/warehouse/dkothari_sqoop_import.db/daily_revenue \
                     --columns order_date,revenue \
                     --input-fields-terminated-by "\001" \
                     --num-mappers 1

video 54 --> sqoop update/ upsert/merge 
video 59 --> command to initiate the spark-shell
       spark-shell --master yarn --conf spark.ui.port=12654 (any 5 digit port no from 10000 to 65535)
      
       Some important spark-shell related aurguments:-->
      
      --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).
      
      Spark standalone and YARN only:
             --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,
                              or all available cores on the worker in standalone mode)
             --num-executors NUM         Number of executors to launch (Default: 2, in yarn mode).
                              
      hadoop / linux command to display size of data -->
              hadoop fs -du -s -h /user/dkothari
                     (here -s is to display size of files and -h is to show human readable format)
              
     spark-shell --master yarn \
       --conf spark.ui.port=12654 \
       --num-executors 1 \
       --executor-memory 512M
       
      All above information fro defaults are available at below listed location:
              under home directory (/user/dkothari)
                     $ view /etc/spark/conf/spark-env.sh
                     $ view /etc/spark/conf/spark-defaults.conf
                     
 command to stop currently running sparkContext -->
              sc.stop
 to initialize or start a new sparkContext now --> //initialize programmatically
       import.org.apache.spark.{sparkConf, sparkContext}
       val conf = new sparkConf().setAppName("Daily Revenue").master("yarn-client")
       val sc = new sparkContext(conf)
 
 Once the spark is up and running --> Below is the command to get the environment related details:
       sc.getConf.getAll.foreach(println)

video 60 --> Spark RDD (Resilient Distributed Dataset) -- lets rewatch it
       RDD --> extension of list dataset. It is in-memory and distributed dataset.
       
       //Validating files from HDFS file system
       hadoop fs -ls /user/dkothari/retail_db/orders
       
       //For simple dataset like list we use index to fetch the data at any specific location. for example, 
              val l = (1 to 100).toList ---> this is a command to create list in scala
              l(5) --> in this way we can fetch record at any specific location.
              
              Question:- 
                     how to get range of values in scala list. 
                     How to fetch last record in the list, if I don't know size. or How to get size of list.
       
      //Create RDD using spark-shell (for HDFS location)
       val orders = sc.textFile("/user/dkothari/retail_db/orders") --> scala command to create RDD and fetch data from text files which is                                                                        located at HDFS location.
       orders.first --> command to get first record from RDD
       orders.take(10) --> command to get top 10 records from RDD
       
              Question:-
                     how to get range of values in scala RDD. 
                     How to fetch last record in the RDD, if I don't know size. or How to get size of RDD.
       
       
                     
       //Read data from local file system
       val ProductsRaw = scala.io.Source.fromFile("/data/retail_db/products/part-00000").getLines.toList 
       
       --> above scala command is to read text file from local file system. "getLines" is the function to fetch each line from file and consider each line as one record. "toList" is function to convert it into scala list dataset.
        
        //Create RDD from local file system
        val productsRDD = sc.parallelize(ProductsRaw) --> parallelize function convert scala list (serialize) into RDD (distributed).

Video 61 --> DAG and Lazy Evaluation

DAG --> Directed Acyclic Graph
Lazy Evaluation --> spark will do lazy evaluation and create DAG till the program hit action. 

its not good practice to use foreach function on RDD directly. foreach functio should be used on scala collection like Array or list.

video 62 --> Reading different file format

       In spark, sparkContext doesn't have any functions to read data directly from different file formats. Hence, we need to use sqlContext API and sqlContext provide various functions to read data from different file format. Although, sqlContext also doesn't support avro file format.
       
       //command to read data from different file format
       sqlContext.read.json 
       sqlContext.read.parquet
       sqlContext.read.orc
       
       sqlContext.read and sqlContext.load --> both of these commands create spark Data Frame.
       
       // Difference between RDD and Dataframe is, RDD is just distributed dataset, while Dataframe is structured distribyted dataset.                     Dataframe is strctured more like a database table.
       val ordersDF = sqlContext.read.json("/user/dkothari/retail_db_json/orders")
       
       //command to preview the data
       ordersDF.show
       // Some Spark Dataframe APIs
       ordersDF.printSchema --> this will print dataframe columns names and datatype
       ordersDF.select("order_id", "order_date").show
       
       //below command is same as sqlContext.read.json
       sqlContext.load("/user/dkothari/reatil_db_json/orders", "json").show
       
video 64 --> spark transformations -- String manipulations
       Need to make sure about data type of original data. 
       Need to know how to type cast data. (toInt, toString, toDouble, etc are some APIs used to typecast)
       
       //below are few command to show how to do string manipulation
       //Create RDD using spark-shell (for HDFS location)
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val str = orders.first
       val A = str.split(',')
       val orderId = A(0).toInt
       val orderDate = A(1)
       val OrderCustID = A(2).toInt
       
       // Some other commands for string manipulation
       A(1).contains("2013")
       A(1).contains("2017")
       A(1).substring(5)
       A(1).substring(0, 9)
       A(1).replace('07','JUL')
       A(1).replace('-','/')
       A(1).indexof("2",2)
       A.length
       
       /Need to understand all these string manipulation function in more details

video 65 --> Spark -- Row level transformations
       
       
       
