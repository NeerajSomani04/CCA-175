All the details are at Itversity.com website under specific courses
       
Video 01 - Scala Fundamentals --> Have instructions on scala setup on mac with intellij IDE
Video 02 - Basic Scala Programming --> Var, val, loops
Video 03 - Functions --> definiing functions and anaonymous funcs.
Video 04 - OOPs concepts --> Class
     below command is used to set Java_Home environment path, so that we can use :javap command of scala.
        export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk
    Few Important points for Scala Class:
       Scala Class defination by default generate default constructor.
       If there is any argumnet of class with Val or Var, then those will be only argument of class and not variable of class.
       If arguments are defined by Val (immutable), getter method will be available by constructor.
       If arguments are defined by Var (mutable), getter and setter method will be available by constructor.
Video 05 - OOPs concepts --> Objects
       Objects are singleton class, hence it can not be intitable more than once. 
       There is no constructor for Objects.
       Companion Objects:- Class name and Object name are same.
Video 06 - OOPs concepts --> Case Classes
      This basically gives you many functionality as a inbuilt, without writting much code.
Video 07 - Scala Fundamentals --> Collection
      Sequence --> hold number of elements
          Array -->
          List -->
      Set --> unique values
      Map --> (Key,value) pair
      
MySql DB Details: (at time of certification, cloudera wouldn't allow to connect to mysql. we need to validate any mysql queries through sqoop)
       users --> 
              retail_user
              hr_user
              nyse_user
       available databases -->
              retail_db
              hr_db
              nyse_db
       hostname --> ms.itversity.com
       password --> itversity
       Commands --> 
              to connect to mysql --> mysql -u retail_user -h ms.itversity.com -p itversity
              to see available databases --> show databases;
              to connect to db --> use retail_db;
              to see tables --> show tables;
       
Sqoop commands:
       how to build connection string for mysql database.
              --> --connect jdbc:mysql://ms.iteversity.com:3306/retail_db
                  where, hostname --> ms.itevrsity.com
                         port --> 3306 (this is usually default port)
                         db_name --> retails_db
       Things to remember for --split-by option of sqoop command:
              1) Column should be indexed
              2) values should be sparse / or it should be sequentially generated or evenly incremented
              3) it should not have null values
              4) we need to use this always when we use --query aurgument
              
       --boundary-query aurgument is used for creating split while importing data.
       --table aurgument should not be used with --query aurgument.
       
Need to start from below video
video 44 -- apache sqoop -- sqoop import -- delimiters and handling nulls
Example Sqoop command:--
            sqoop import \
            --connect jdbc:mysql://ms.itversity.com:3306/hr_db \
            --username hr_user \
            --password itversity \
            --table employees \
            --warehouse-dir /user/dkothari/sqoop_import/hr_db \
            --null-non-string -1 \
            --fields-terminated-by "\t" \
            --lines-terminated-by ":"
            
video 47 -- Sqoop command for Hive import --
       sqoop import \
       --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
            --username retail_user \
            --password itversity \
            --table order_items \
            --hive-import \
            --hive-database dkothari_sqoop_import \
            --hive-table order_items \
            --num-mappers 2


sqoop import \
       --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
            --username retail_user \
            --password itversity \
            --table orders \
            --hive-import \
            --hive-database dkothari_sqoop_import \
            --hive-table orders \
            --hive-overwrite \
            --num-mappers 2
            
video 50 -->
       Question : Create Hive table "Daily_Revenue". based on the data fetched from other tables.
       Command: --> 
              create table daily_revenue as
              select order_date, sum(order_item_subtotal) daily_revenue
              from orders join order_items on 
              order_id = order_item_order_id
              where order_date like '2013-07%'
              group by order_date;
video 51 --> Sqoop export -- create hive table
       mysql command for creating table in retail_export --> create table daily_revenue_dkothari
              (order_date varchar(30), revenue float);
     
       sqoop export command-->
              sqoop export \
                     --connect jdbc:mysql://ms.itversity.com:3306/retail_export \
                     --username retail_user \
                     --password itversity \
                     --table daily_revenue_dkothari \
                     --export-dir /apps/hive/warehouse/dkothari_sqoop_import.db/daily_revenue \
                     --input-fields-terminated-by "\001"  
video 53 --> 
       mysql demo table --> 
              create table daily_revenue_dkothari_demo
              (revenue float, order_date varchar(30), description varchar(200));
     
      sqoop export command -->
              --column aurgument is very important. The order of column names in column aurgument should be same as source table                                 structure (which hdfs directory or hive table). Below is example:
              
              sqoop export \
                     --connect jdbc:mysql://ms.itversity.com:3306/retail_export \
                     --username retail_user \
                     --password itversity \
                     --table daily_revenue_dkothari_demo \
                     --export-dir /apps/hive/warehouse/dkothari_sqoop_import.db/daily_revenue \
                     --columns order_date,revenue \
                     --input-fields-terminated-by "\001" \
                     --num-mappers 1

video 54 --> sqoop update/ upsert/merge 
video 59 --> command to initiate the spark-shell
       spark-shell --master yarn --conf spark.ui.port=12654 (any 5 digit port no from 10000 to 65535)
      
       Some important spark-shell related aurguments:-->
      
      --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).
      
      Spark standalone and YARN only:
             --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,
                              or all available cores on the worker in standalone mode)
             --num-executors NUM         Number of executors to launch (Default: 2, in yarn mode).
                              
      hadoop / linux command to display size of data -->
              hadoop fs -du -s -h /user/dkothari
                     (here -s is to display size of files and -h is to show human readable format)
              
     spark-shell --master yarn \
       --conf spark.ui.port=12654 \
       --num-executors 1 \
       --executor-memory 512M
       
      All above information fro defaults are available at below listed location:
              under home directory (/user/dkothari)
                     $ view /etc/spark/conf/spark-env.sh
                     $ view /etc/spark/conf/spark-defaults.conf
                     
 command to stop currently running sparkContext -->
              sc.stop
 to initialize or start a new sparkContext now --> //initialize programmatically
       import.org.apache.spark.{sparkConf, sparkContext}
       val conf = new sparkConf().setAppName("Daily Revenue").master("yarn-client")
       val sc = new sparkContext(conf)
 
 Once the spark is up and running --> Below is the command to get the environment related details:
       sc.getConf.getAll.foreach(println)

video 60 --> Spark RDD (Resilient Distributed Dataset) -- lets rewatch it
       RDD --> extension of list dataset. It is in-memory and distributed dataset.
       
       //Validating files from HDFS file system
       hadoop fs -ls /user/dkothari/retail_db/orders
       
       //For simple dataset like list we use index to fetch the data at any specific location. for example, 
              val l = (1 to 100).toList ---> this is a command to create list in scala
              l(5) --> in this way we can fetch record at any specific location.
              
              Question:- 
                     how to get range of values in scala list. 
                     How to fetch last record in the list, if I don't know size. or How to get size of list.
       
      //Create RDD using spark-shell (for HDFS location)
       val orders = sc.textFile("/user/dkothari/retail_db/orders") --> scala command to create RDD and fetch data from text files which is                                                                        located at HDFS location.
       orders.first --> command to get first record from RDD
       orders.take(10) --> command to get top 10 records from RDD
       
        Question:-
              how to get range of values in scala RDD. 
              How to fetch last record in the RDD, if I don't know size. or How to get size of RDD.
       
       
                     
       //Read data from local file system
       val ProductsRaw = scala.io.Source.fromFile("/data/retail_db/products/part-00000").getLines.toList 
       
       --> above scala command is to read text file from local file system. "getLines" is the function to fetch each line from file and consider each line as one record. "toList" is function to convert it into scala list dataset.
        
        //Create RDD from local file system
        val productsRDD = sc.parallelize(ProductsRaw) --> parallelize function convert scala list (serialize) into RDD (distributed).

Video 61 --> DAG and Lazy Evaluation

DAG --> Directed Acyclic Graph
Lazy Evaluation --> spark will do lazy evaluation and create DAG till the program hit action. 

its not good practice to use foreach function on RDD directly. foreach functio should be used on scala collection like Array or list.

video 62 --> Reading different file format

       In spark, sparkContext doesn't have any functions to read data directly from different file formats. Hence, we need to use sqlContext API and sqlContext provide various functions to read data from different file format. Although, sqlContext also doesn't support avro file format.
       
       //command to read data from different file format
       sqlContext.read.json 
       sqlContext.read.parquet
       sqlContext.read.orc
       
       sqlContext.read and sqlContext.load --> both of these commands create spark Data Frame.
       
       // Difference between RDD and Dataframe is, RDD is just distributed dataset, while Dataframe is structured distribyted dataset.                     Dataframe is strctured more like a database table.
       val ordersDF = sqlContext.read.json("/user/dkothari/retail_db_json/orders")
       
       //command to preview the data
       ordersDF.show
       // Some Spark Dataframe APIs
       ordersDF.printSchema --> this will print dataframe columns names and datatype
       ordersDF.select("order_id", "order_date").show
       
       //below command is same as sqlContext.read.json
       sqlContext.load("/user/dkothari/reatil_db_json/orders", "json").show
       
video 64 --> spark transformations -- String manipulations
       Need to make sure about data type of original data. 
       Need to know how to type cast data. (toInt, toString, toDouble, etc are some APIs used to typecast)
       
       //below are few command to show how to do string manipulation
       //Create RDD using spark-shell (for HDFS location)
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val str = orders.first
       val A = str.split(',')
       val orderId = A(0).toInt
       val orderDate = A(1)
       val OrderCustID = A(2).toInt
       
       // Some other commands for string manipulation
       A(1).contains("2013")
       A(1).contains("2017")
       A(1).substring(5)
       A(1).substring(0, 9)
       A(1).replace('07','JUL')
       A(1).replace('-','/')
       A(1).indexof("2",2)
       A.length
       
       /Need to understand all these string manipulation function in more details

video 65 --> Spark -- Row level transformations (map function)
        // few frequently used API for this:
              map, flatMap, mapPartitions, mapPartitionsWithIndex
        
     Exercise:- Extract date from orders data (which is in "1,2013-07-25 00:00:00.0,11599,CLOSED" format) and convert "2013-07-25 00:00:00.0" date into 20130725 as Int format.
     Solution:- //commands below
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val orderDates = orders.map((str: String) => str.split(',')(1).substring(0,10).replace("-","").toInt)
       
       val ordersPairRDD = orders.map( (order: String) => {
                val o = order.split(',')
                (o(0).toInt, o(1).substring(0,10).replace("-","").toInt)
               })
              
        val orderItems = sc.textFile("/user/dkothari/retail_db/order_items")
        val orderItemsPairRDD = orderItems.map ( orderItem => {
              (orderItem.split(",")(1).toInt, orderItem)
              })

video 66 --> Spark -- Row level transformations (flatMap function)
       // Need to create a scenario to perform this. Hence, creating a list of word count problem.
       val l = List("Hello", "How are you doing", "let us perform word count", "As part of the word count problem", "we will see how many times each word repeat")
       val l_rdd = sc.parallelize(l)
       //Below command will split each record of RDD and then again split each record with " " (space char)) //
       val l_flatMap = l_rdd.flatMap( ele => ele.split(" ")) //this command run on each input item and split them into one or more
       val word_count = l_flatMap.map( word => (word,1)).countByKey
       
       //try to run below command to understand the diffenece between map and flatMap
       val l_map = l_rdd.map( ele => ele.split(" ")) //this command run on each input item and create each output
       
video 67 -- spark -- filter function
       // row level filtering:
       val orderFiltered = orders.filter(order => order.split(",")(3) == "COMPLETE")
       orderFiltered.count // this command is used to get the count of records in RDD
       
       Exercise:- Get all the orders of 2013-09 which are in closed or complete status
       Solution:- val orderFiltered = orders.filter(order => ((order.split(",")(3) == "COMPLETE" || order.split(",")(3) == "CLOSED") && 
                            order.split(",")(1).contains("2013-09"))
                            )
       orderFiltered.take(10).foreach(println)
       orderFiltered.count
       
       //below is the command to get distinct values of order status
       orders.map(order => order.split(",")(3).distinct
       
video 68 -- spark -- join function
       // join also support leftOuterJoin , rightOuterJoin, fullOuterJoin
       // joining orders and order_items data
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val orderItems = sc.textFile("/user/dkothari/retail_db/order_items")
       val ordersMap = orders.map(order => { 
              (order.split(",")(0).toInt, order.split(",")(1).substring(0,10))
              })
       val orderItemsMap = orderItems.map( orderItem => {
              (orderItem.split(",")(1).toInt, orderItem.split(",")(4).toFloat)
              })
       
       // take(10).foreach(println)
       val ordersJoin = ordersMap.join(orderItemsMap)

video 69 -- spark -- join // outerJoin
       Exercise: (problem statement) --> Get all the orders which do not have corresponding entries in order_items table
       Solution: 
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val orderItems = sc.textFile("/user/dkothari/retail_db/order_items")
       val ordersMap = orders.map(order => { 
              (order.split(",")(0).toInt, order)
              })
       val orderItemsMap = orderItems.map( orderItem => {
              (orderItem.split(",")(1).toInt, orderItem)
              })
       val ordersLeftOuterJoin = ordersMap.leftOuterJoin(orderItemsMap)
       val ordersLeftOuterJoinFilter = ordersLeftOuterJoin.filter( oi => oi._2._2 == None)
       val ordersWithNoOrderItems = ordersLeftOuterJoinFilter.map( o => o._2._1)
  
video 71 -- spark -- Aggregations -- using Actions APIs
      
       Exercise statment:- get the count of orders by order status 
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val ordersCountMap = orders.map( order => (order.split(",")(3), 1)).countByKey
       ordersCountMap.take(10).foreach(println)
       
       Exercise:- compute revenue for the month of 2013-09
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val orderItems = sc.textFile("/user/dkothari/retail_db/order_items")
       val orderItemsRevenue = orderItems.map( oi => oi.split(",")(4).toFloat)
       val orderItemsRevenueTotal = orderItemsRevenue.reduce((total, revenue) => total+revenue)
       
video 72 -- spark -- Aggregations -- combiner
      // differenece between groupByKey, reduceByKey, aggregateByKey APIs
      groupByKey --> it doesn't use combiner. Its performance is slower than reduceByKey and aggregateByKey
              1, (1 to 1000) -- sum(1 to 1000) => 1 + 2 + 3 + 4...1000
          
      reduceByKey --> it uses concept of combiner. which means aggregating based on combining result from intermediate values. But we need                              to use reduceByKey only when logic for all the intermediate computation and final compunatation is same. for                              example: (sum, count, etc). as you can see in syntext of reduceByKey, it uses only one func as aurgument to                                   compute the result.
              1, (1 to 1000) -- sum(1 to 1000) => (sum(1,250), sum(251,500), sum(501,750), sum(751,1000))
      aggregateByKey --> it is same as reduceByKey, the only difference is you can use it even if the computation logic for intermediate                                stage is different. for example (average, complex computation). As you can see in syntext of aggregateByKey, it uses                                two func as aurgument, one for intermediate computation and one for final combining computation to compute                                the final result. 
 
 video 73 -- spark -- groupByKey function
       Exercise statement:- get revenue by order_id 
       Solution:
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val orderItems = sc.textFile("/user/dkothari/retail_db/order_items")
       
       val ordersMap = orders.map(order => { 
              (order.split(",")(0).toInt, order)
              })
       val orderItemsMap = orderItems.map(orderItem => {
              (orderItem.split(",")(1).toInt, orderItem.split(",")(4).toFloat)
              })
       val orderItemsGBK = orderItemsMap.groupByKey
       val orderItemsRevenue = orderItemsGBK.map(rec => (rec._1,rec._2.toList.sum))
       
       Exercise statement:- Get data in descending order by order_item_subtotal for each order_id
       val ordersSortedByRevenue = orderItemsGBK.flatMap(rec => {
                                                       rec._2.toList.sortBy(o => -o).map(k => (rec._1, k))
                                                    })
                                                    
video 74 -- Aggregations -- reduceByKey
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val orderItems = sc.textFile("/user/dkothari/retail_db/order_items")
       val orderItemsMap = orderItems.map(orderItem => {
              (orderItem.split(",")(1).toInt, orderItem.split(",")(4).toFloat)
              })
       val RevenuePerOrderId = orderItemsMap.reduceByKey((total, revenue) => total+revenue)

video 75 -- Aggregations -- aggregateByKey
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val orderItems = sc.textFile("/user/dkothari/retail_db/order_items")
       val orderItemsMap = orderItems.map(orderItem => {
              (orderItem.split(",")(1).toInt, orderItem.split(",")(4).toFloat)
              })
       //input data format --> (order_id, order_item_subtotal)
       //output data format --> (order_id, (order_revenue, order_item_subtotal))
       
       val revenueAndMaxPerOrderID = orderItemsMap.aggregateByKey((0.0f,0.0f))(
       (inter, subtotal) => (inter._1+subtotal, if(subtotal > inter._2) subtotal else inter._2,
       (total, inter) => (total._1 + inter._1, if(total._2 > inter._2) total._2 else inter._2)
       )
       
      Homework Exercise Statement:- calculate revenueAndMinPerOrderId by looking at above solution. there will be some tweak and                                        challanges. its not same as revenueAndMaxPerOrderID calculation.

video 76 -- Spark core API -- SortByKey
       // Problem statement 1 -- Products Sorted by CategoryId
        val products = sc.textFile("/user/dkothari/retail_db/products")
        val productsMap = products.map(product => {
              (product.split(",")(1).toInt, product)
              })
        val productsSorted = productsMap.sortByKey()
        
         // Problem statement 2 -- Products Sorted by composite key 
                                   (ascending order by product_categoryId and descending order by product_price)
         val products = sc.textFile("/user/dkothari/retail_db/products")
         val productsMap = products.map(product => {
              ((product.split(",")(1).toInt, product.split(",")(4).toFloat), product)
              })
         
         While running the above Map command we found Error, (exception, NumberFormatException)
         To understand the issue as explained in video we need to do the data analysis.
         In video, below logic is one way to see the data and understand the reason for error:
         
         //so reason for error is, there is comma (,) in the 3rd data field itself. 
         val productFilterForError = products.filter(product => product.split(",")(4) == "")
         
         // So, we need to filter this record
         val productsMap = products.filter(product => product.split(",")(4) != "").
                            map(product => {
              ((product.split(",")(1).toInt, -product.split(",")(4).toFloat), product)
              })
         // In above statement if you see I neget the second part because its data type is float and we need product_price in decending order we can do it this way. but it cant be done with string data type. for string data type we need to use some other API logic like groupBy, etc.
         //Below is the command to get only the data field and not the key
         val productsSorted = productsMap.sortByKey().map(rec=> rec._2)
         productsSorted.take(10).foreach(println)


video 77 -- Spark -- API -- Ranking -- SortByKey and TakeOrdered
         //problem statement -- Get details of top 10 products by Price
         val products = sc.textFile("/user/dkothari/retail_db/products")
         val productsMap = products.
               filter(product => product.split(",")(4) != "").
               map(product => (product.split(",")(4).toFloat, product))
              
         val productsSorted = productsMap.sortByKey(false)
         
         //Below solution done useing TakeOrdered API
         val products = sc.textFile("/user/dkothari/retail_db/products")
         val productsMap = products.
               filter(product => product.split(",")(4) != "").
               takeOrdered(10)(Ordering[Float].reverse.on(product => product.split(",")(4).toFloat))
         val productsSortedByPrice = productsMap.foreach(println)
         
         Homework Problem statement:- Get details of top 10 products and get details of top 10 priced products. 
              //(Hint:- top 10 priced product may have more than 10 records)

video 78 -- spark -- core API - Ranking -- GroupByKey -- Products Per Category
              // Get top N priced product within each category
              val products = sc.textFile("/user/dkothari/retail_db/products")
              val productsMap = products.
                 filter(product => product.split(",")(4) != "").
                 map(product => (product.split(",")(1).toInt, product))
              val ProductsGrpByCategory = productsMap.groupByKey
              
video 79 -- spark -- core API -- Products per Category (continuation of above video) -- part 1 (getTopNPrices)
       // Here to understand it more accurately, we are dividing complete problem into multiple steps 
       //command to get first records collection inorder to understand data
              val productsIterable = ProductsGrpByCategory.first._2
              productsIterable.take(10).foreach(println) // this will show us all the products into that specific category
              productsIterable.size() // this will give us no of products (size of iterable object) into that category
       // By looking at result of above command, now we need to build logic to get top 5 products for that specific category
           
           //So, lets break our problem once again and get top 5 products prices first
               val productPrices =  productsIterable.map(p => p.split(",")(4).toFloat).toSet 
           // above command will retrive prices and we used set to remove duplicate value of prices
           // Now to get top 5 product prices from above result
              val topNProductPrices = productPrices.toList.sortBy(p => -p).take(5)
           //we converted set collection again to list because we want to use SortBy API. 
              
video 80 -- spark -- core API -- Products per Category (continuation of above video) -- part 2 (getTopNPricesProducts)          
           //Now, we need to get all the products in descending order by price
           val productsSortedByPrice = productsIterable.toList.sortBy(product => -product.split(",")(4).toFloat)
           val minOfTopNPrices = topNProductPrices.min
           val topNPricedProducts = productsSortedByPrice.takeWhile(product => product.split(",")(4).toFloat >= minOfTopNPrices)
           
           // why we didn't use filter API in above statement. because our data is already sorted so we don't need to go through all the records. takeWhile API will stop iterating once it will reach the specified condition.
           
           
           // Lets create a scala function to get top N product
           
           def getTopNProducts(productsIterable: Iterable[String], topN: Int): Iterable[String] = {
                val productPrices =  productsIterable.map(p => p.split(",")(4).toFloat).toSet 
                val topNProductPrices = productPrices.toList.sortBy(p => -p).take(topN)
                val productsSortedByPrice = productsIterable.toList.sortBy(product => -product.split(",")(4).toFloat)
                val minOfTopNPrices = topNProductPrices.min
                val topNPricedProducts = productsSortedByPrice.takeWhile(product => product.split(",")(4).toFloat >= minOfTopNPrices)
                topNPricedProducts
           }
   
video 81 -- spark -- core API -- Products per Category (continuation of above video) -- part 3 (getTopNPricesProductsByCategory)
       // Now we need to understand how we wants to get our output and based on that we need to decide which API we wants to use
       // spark map API will be used --> when you want to retrive RDD collection
       // spark flatMap API will be used --> when you want to retrive individual records 

        val top3PricedProductsByCategory = ProductsGrpByCategory.flatMap( rec => (getTopNProducts(rec._2, 3))
        
video 82 -- spark -- core API -- setOperations
      sets --> are the collection of elements with similar data types and unique values
      set operations -->
              unique --> all unique values of all sets
              intersects --> common values betten sets
              difference/ minus --> 
              distinct --> 
      
      problem statement --> Get all customers who placed orders in Aug 2013 and Sep 2013
      
      val orders = sc.textFile("/user/dkothari/retail_db/orders")
      val customers_201308 = orders.
              filter(order => order.split(",")(1).contains("2013-08")).
              map(order => order.split(",")(2).toInt)
      
      customers_201308.distinct.count // by this command we can see how many unique custmers placed orders in 201308.
      
      val customers_201309 = orders.
              filter(order => order.split(",")(1).contains("2013-09")).
              map(order => order.split(",")(2).toInt)
      solution:
      
      val customers_201308_and_201309 = customers_201308.intersection(customers_201309)
      
      problem statement --> Get all unique customers who placed orders in Aug 2013 or Sep 2013
      Solution: 
      val customers_201308_union_201309 = customers_201308.union(customers_201309)
      customers_201308_union_201309.distinct.count
      
      problem statement --> Get all unique customers who placed orders in Aug 2013 but not in Sep 2013
      Solution:
      val customers_201308_minus_201309 = customers_201308.map( c => (c,1)).
              leftOuterJoin(customers_201309.map(c => (c,1))).
              filter( rec => rec._2._2 == None).
              map( rec => rec._1)
              
      customers_201308_minus_201309.distinct.count
      customers_201308_minus_201309.take(10).foreach(println)

video 83 -- spark -- core API -- save RDD in text File format and with delimiter
       // save RDD back to HDFS
       
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val orderCountByStatus = orders.
              map( order => (order.split(",")(3), 1)).countByKey // this will outpout scala map but it doesn't have saveAsTextFile API
       
       val orderCountByStatus = orders.
              map(order => (order.split(",")(3), 1)).
              reduceByKey((total,ele) => total+ele) // reduceByKey is used to so that we can use saveAsTextFile API.
       
       orderCountByStatus.saveAsTextFile("/user/dkothari/test_save_file/orders_count_by_status")
       hadoop fs -cat /user/dkothari/test_save_file/orders_count_by_status/part-00000 // command to view data
       
       orderCountByStatus.
              map(rec => rec._1 + "\t" + rec._2).
              saveAsTextFile("/user/dkothari/test_save_file_1/orders_count_by_status") 
              // this above is the same command as previous but with tab ("\t") as delimiter in the output file. We need to define delimilter only in the case of text file. for Json, parquet and other file we will see spark-sql.
       
       //Below is the command to preview the data
       sc.textFile("/user/dkothari/test_save_file_1/orders_count_by_status").take(10).foreach(println)
       
video 84 -- spark -- core API -- save RDD in textFile format, delimiter and compression
      orderCountByStatus.saveAsTextFile("/user/dkothari/test_save_file_compression/orders_count_by_status", classOf[org.apache.hadoop.io.compress.SnappyCodec])
      
      sc.textFile("/user/dkothari/test_save_file_compression/orders_count_by_status").take(10).foreach(println)
      

video 85 -- spark -- core API -- save RDD data in different file format
       orc, json, parquet, avro
       Steps below:
              1) Data should be in spark Data Frame
              2) use write or Save API to save data frame in different file format
              3) we can also use compression algorithm if required
      
      val orderDF = sqlContext.read.json("/user/dkothari/retail_db_json/orders") //command to read json file and create data frame
      orderDF.write.orc("/user/dkothari/orders_orc") //command to write data into orc file format
      
      val orderDF = sqlContext.load("/user/dkothari/retail_db_json/orders", "json") // this command can also be use to read json file and create DF
      orderDF.save("/user/dkothari/orders_parquet", "parquet") //command to save data in parquet format
      
      sqlContext.load("/user/dkothari/orders_parquet", "parquet").show //command to view data after saving


video 86 -- spark -- Practice Problem statement -- understanding
       1) use retail_db
       2) problem statement 
              i) get daily revenue by product considering completed and closed orders
              ii) data needs to be sorted in ascending order by order_date and then descending order by revenue computed for each product for each day
              iii) data should be delimited by "," in this order --
                       order_date, daily_revenue_per_product, product_name
       3) data for orders and order_items are available in HDFS --
              /public/retail_db/orders and /public/retail_db/order_items
       4) data for products is available locally under --
              /data/retail_db/products
       5) Final output needs to be stored under --
              HDFS location - avro format:
              /user/dkothari/daily_revenue_avro_scala
              HDFS location - text format:
              /user/dkothari/daily_revenue_txt_scala
              Local location - text format:
              /home/dkothari/daily_revenue_txt_scala
              Solution needs to be stored under;
              /home/dkothari/daily_revenue_scala.txt
 
 
 video 87 -- spark -- Solution Launch spark shell
 
 
       // first lets see the size of data file
       hadoop fs -ls -h /public/retail_db/orders // 2.9 MB
       hadoop fs -ls -h /public/retail_db/order_items // 5.2 MB
       
       // now we need to understand the capacity of cluster, like number of executers, size of blocks, etc
       //cloudera will provide resource manager web URL like itversity --> (rm01.itversity.com:8088)
       Couple of thing to understand and remember before we launch the spark-shell. We need to understand teh resources available on cluster. Based on that we can decide and launch spark-shell with number of executors.
       i) No of resources/executor available
       ii) size of file that we are trying to process
       
      once you are on resourcemanager web URL: look for 2 important properties:
              1) Memory Total --> this is summation of multiplication of all Vcores and executor memory. [ sum( vcores * memory)]
              2) VCores Total --> this means no of processors or executers.
              3) executor memory allocation --> minimum and maximum allowable memory.
      
      1) Launch spark shell with proper configuration
      spark-shell --master yarn \
              --num-executors 12 \
              --executor-memory 2G \
              --conf spark.ui.port=12855 //this conf is only for itversity lab. for cloudera it might not be important.
      
video 88 -- spark -- Solution -- read, filter and join data    
      2) Read data: 
              val orders = sc.textFile("/public/retail_db/orders")
              val orderItems = sc.textFile("/public/retail_db/order_items")
           //its important to understand the data by previewing right away 
              orders.first
              orderItems.first
              orders.take(10).foreach(println)
              orderItems.take(10).foreach(println)
          
          // for exam day tip --> Durga sir mentioned that their will be different datasets and not only one like retail_db. Although, questions will be much simpler than what we are practicing in video series.

    3) Filter data for completed and closed orders (as per requirement)
          // when you apply any spark API, you need to make sure the datatype of RDD and its aurguments.
          // So, to understand data better and different values of order_status, we use below command
          orders.map(order => order.split(",")(3)).distinct.take(10).foreach(println)
          
       val ordersFiltered = orders.
              filter( order => order.split(",")(3) == "COMPLETE" || order.split(",")(3) == "CLOSED")
              
       ordersFiltered.take(10).foreach(pirntln) //command to view the data
       
      4) Convert both filtered orders and order_items into (key, value) pair, so that we can join both the dataset later
          
          val ordersMap = ordersFiltered.map( order => (order.split(",")(0).toInt, order.split(",")(1)))
          
          val orderItemsMap = orderItems.
              map(orderItem => (orderItem.split(",")(1).toInt, (orderItem.split(",")(2).toInt, orderItem.split(",")(4).toFloat)))
          
          ordersMap.first
          orderItemsMap.first
          
        5) join dataset to calculate revenue
              val ordersJoin = ordersMap.join(orderItemsMap)
              ordersJoin.first // to view data
          //output of join will look like this (order_id, (order_date, (order_item_product_id, order_item_subtotal)))
          
video 89 -- spark -- Solution -- calculate daily revenue per product
        6) calculate daily revenue per product_id
             //In order to calculate this we need to transform data into below format
             // ((order_date, order_item_product_id), order_item_subtotal)
             // ((order_date, order_item_product_id), daily_revenue_per_product_id) //here we did calculation
             //below is the code to get desired output here
             
             val ordersJoinMap = ordersJoin.
                                   map( rec => ((rec._2._1, rec._2._2._1), rec._2._2._2))
             val daily_revenue_per_product_id = ordersJoinMap.
                                                   reduceByKey((revenue, order_item_subtotal) => revenue + order_item_subtotal)
             daily_revenue_per_product_id.first // to view the data
             daily_revenue_per_product_id.count // to get the count of data items
             
video 90 -- spark -- Solution -- read and join products data that is available at local file system

         7) read products data from local file system and create RDD
              // sc.parallelize -- is used to create RDD from local files
              // when we use scala.io API to read the file from local file system, we need to give fully qualified path of each file. It is not possible to read all the files under one specific directory by just giving directory name. 
              // file path should be given like this ("/data/retail_db/products/part-00000")
              // we have read each file seperately, update the collection and use sc.parallelize to create RDD.
              
              import scala.io.Source
              val productsRaw = Source.fromFile("/data/retail_db/products/part-00000").getLines
              
              //now you can see "productsRaw" object is of type Iterator[String]
              //Also, sc.parallelize() can not convert Iterator object into RDD. 
              //sc.parallelize() requires object of type sequence, (like List, Set, etc.)
              //below are the commands
              
               val productsRaw = Source.fromFile("/data/retail_db/products/part-00000").getLines.toList
               val products = sc.parallelize(productsRaw)
               //below are the commands to preview data
               products.first
               products.count
               products.take(10).foreach(println)
       8) join daily_revenue_per_product_id with products to get daily_revenue_per_product (by name)
               val productsMap = products.
                                     map( product => (product.split(",")(0).toInt, product.split(",")(2)))
               
               val daily_revenue_per_product_id_Map = daily_revenue_per_product_id.
                                     map( rec => (rec._1._2, (rec._1._1, rec._2)))
                                     
               daily_revenue_per_product_id_Map.first //to preview the data
               
               val dailyRevenuePerProductJoin = daily_revenue_per_product_id_Map.join(productsMap)
               
               dailyRevenuePerProductJoin.first //to view and verify data
               
 video 91 -- spark -- Solution -- Sort and save data is desired format
       9) sort the data in ascending order by order_date and daily_revenue_per_product in descending order
               // "dailyRevenuePerProductJoin"  this object is has data in below format
               //(product_id, ((order_date, daily_revenue), product_name))
               // but we need data in below format and it should be sorted properly
               //(order_date, daily_revenue, product_name)
               (24,((2013-07-25 00:00:00.0,319.96),Elevation Training Mask 2.0))
               val dailyRevenuePerProductJoinMap = dailyRevenuePerProductJoin.
                          map( rec => ((rec._2._1._1, -rec._2._1._2), (rec._2._1._1, rec._2._1._2, rec._2._2)))
               val dailyRevenuePerProductJoinMapSorted = dailyRevenuePerProductJoinMap.sortByKey()
               dailyRevenuePerProductJoinMapSorted.take(50).foreach(println)//to view data
               
       10) Get data in desired format -- order_date, daily_revenue, product_name
              // "mkString" is the scala function that can be applied to any collection, to convert all element of each record into one single string and you can also apply delimiter while forming your string. As you can see below code.
              // Below command will not work because it requires all the elements of the records in data type String. In our case, daily_revenue is of type Float. So, we need to do some changes in below code or use alternative approach.
              
              val dailyRevenuePerProduct = dailyRevenuePerProductJoinMapSorted.
                            map( rec => rec._2.mkString(","))
                            
               
               //As an alternative approach, we will use below code
                val dailyRevenuePerProduct = dailyRevenuePerProductJoinMapSorted.
                            map( rec => rec._2._1 + "," + rec._2._2 + "," + rec._2._3)
                
                dailyRevenuePerProduct.take(10).foreach(println)
                
      11) save data into HDFS location in text file format
                 dailyRevenuePerProduct.saveAsTextFile("/user/dkothari/daily_revenue_txt_file")
                 sc.textFile("/user/dkothari/daily_revenue_txt_file").take(10).foreach(println)
      12) save data into local file system in text file format   
            // below will copy all the files under specified directory
                 hadoop fs -get /user/dkothari/daily_revenue_txt_file /home/dkothari/daily_revenue_txt_scala
            //if the requirement is to copy entire directory then use below command
            //first we need to make sure the required directory is available at specific location, if not create it
               mkdir /home/dkothari/daily_revenue_scala_2
               hadoop fs -get /user/dkothari/daily_revenue_txt_file /home/dkothari/daily_revenue_scala_2/daily_revenue_txt_scala
                 
 video 92 -- spark -- Spark SQL -- Different Interface               
           // many tools available for spark SQL in different cluster environment.
            //Below is the command. this command will work in labs.itversity.com or HortonWorks. Not in Cloudera.
            
           1)  spark-sql --master yarn --conf spark.ui.port=12876
            show databases; //command to view all available data bases.
            //In spark-sql query will compile into spark framework.
           
           2) // Hive is another tool
              hive --> just this command will lauch hive tool.
              //in the query will compile in map-reduce framework.
              
  video 93 -- spark -- Spark SQL -- Create Hive table and load data from and to Text File Format
          1) Create Database dkothari_retail_db_txt
          2) Create orders and Order_items table in dkothari_retail_db_txt database
          3) Load data into tables -- Data needs to be preformatted as per the table structure defined
          
          create database dkothari_retail_db_txt; 
          use dkothari_retail_db_txt;
          dfs -ls; // this command is equivalent to hadoop fs -ls in hive
          set hive.metastore.warehouse.dir; //this command is used to verify hive base dir path
          // hive.metastore.warehouse.dir=/apps/hive/warehouse
          
          dfs -ls /apps/hive/warehouse; // to list all available databases under hive.
          
           // In hive we don't use indexes or primary key. Its not reliable.
           
           create table orders (
              order_id int,
              order_date string,
              order_customer_id int,
              order_status string) row format delimited fields terminated by ','
              stored as textfile;
              
          // textfile is default file format.
          show tables; //to list all available tables in db.
          select * from orders limit 10; //to sleect first 10 records from table.
             
          describe formatted orders; //this is a command to see defination of table
          
          // to load data from text file at local file system to hive table, below is the commmand
          load data local inpath '/data/retail_db/orders' into table orders;
          
          // to load data from text file at HDFS file system location to hive table, below is the commmand
          load data inpath '<hdfs file path location>' into table orders;
         
         // we need to be very careful with load command as it doesn't perform any cross validation.
         
          dfs -ls /apps/hive/warehouse/dkothari_retail_db_txt.db/orders;
          
          create table order_items (
              order_item_id int,
              order_item_order_id int,
              order_item_product_id int,
              order_item_quantity int,
              order_item_subtotal float,
              order_item_product_price float) row format delimited fields terminated by ','
              stored as textfile;
          
          load data local inpath '/data/retail_db/order_items' into table orders_items;

video 94 -- spark -- Spark SQL -- create database, tables and load data in ORC File Format
       
       create database dkothari_retail_db_orc; 
       use dkothari_retail_db_orc;
       
       create table orders (
              order_id int,
              order_date string,
              order_customer_id int,
              order_status string)
              stored as textfile;
              
       create table order_items (
              order_item_id int,
              order_item_order_id int,
              order_item_product_id int,
              order_item_quantity int,
              order_item_subtotal float,
              order_item_product_price float)
              stored as textfile;
              
       // now inorder to load data from local text file format to orc table in Hive, it will be 2 step process.
             // 1) Insert data in Hive table (call it as stage table) into the same format as of input text file format.
             // 2) copy data from this stage hive table to needed target orc table format.
       //below is one way of doing so
       
       insert into table dkothari_retail_db_orc.orders select * from dkothari_retail_db_txt.orders;
       insert into table dkothari_retail_db_orc.order_items select * from dkothari_retail_db_txt.order_items;
       
       select * from dkothari_retail_db_orc.orders limit 10;
       select * from dkothari_retail_db_orc.order_items limit 10;

video 95 -- spark -- Spark SQL -- Using spark-shell
       // we can write same queries that we used in Hive interface using spark-shell as well.
       // below are the syntext
       launch spark-shell --> spark-shell --master yarn --conf spark.ui.port=12876
       sqlContext.sql("any valid hive query")
       sqlContext.sql("select * from orders limit 10").show // show will print the result onto consol
 
video 96 -- spark -- Spark SQL -- Hive Functions
       show functions; // command to see some functions.
      // like  Aggregate functions (sum, count, min, max, etc)
      // String functions (substr/subString, instr, rlike, like, length, lcase or lower, ucase or upper, trim, ltrim, rtrim, lpad, rpad, cast, split etc)
      // date functions 
      
      describe function length; // this command will show syntext and defination of function
      select length('Hello World'); // this is the way to use the function.
      select length(order_status) from orders limit 10; // this is to use function on table
      select order_status, length(order_status) from orders limit 10;
      
video 97 -- spark -- Spark SQL -- Hive Functions -- manipulating strings
       // lets first create customer table in hive
       create table customers (
          customer_id int,
          customer_fname varchar(45),
          customer_lname varchar(45),
          customer_email varchar(45),
          customer_password varchar(45),
          customer_street varchar(45),
          customer_city varchar(45),
          customer_state varchar(45),
          customer_zipcode varchar(45)) row format delimited fields terminated by ','
          stored as textfile;
      
      load data local inpath '/data/retail_db/customers' into table customers;
      
      select substring('Hello World, How are you', 14);
      select substring('Hello World, How are you', 7, 5); // result is string 'World'
      select substr('Hello World, How are you', -3); //print last 3 char 'you'
      select substr('Hello World, How are you', -7, 3); // result will be 'are'
      select instr('Hello World, How are you', ' '); // 6 index position number
      select like('Hello World, How are you', 'Hello'); // this is a wrong syntext, below is correct one
      select 'Hello World, How are you' like 'Hello%'; // similar to data base commands
      select lower('Hello World');
      select lcase('Hello World');
      select initcap('hello world'); // this function will intialize every string will capital letter
      select trim(' hello world '); // it will trim left and right space charachter.
      select lpad(12, 3, '0'); // result will be 012
      select cast('12' as int); //it type cast any sting value
      select cast(substr(order_date, 6, 2) as int) from orders limit 10;
      select split('Hello World, how are you', ' '); // create array by spliting data
      select index(split('Hello World, how are you', ' '), 4); // access value of split data


video 98 -- spark -- Spark SQL -- Hive Functions -- manipulating Date 
      Below are few Hive Date functions:
      current_date --> select current_date;
      current_timestamp --> 
      date_add --> select date_add('2017-05-23', 1);
      date_format --> select date_format('2017-05-23', 'y');
      date_sub
      datediff
      day / dayofmonth
      to_date
      to_unix_timestamp
      to_utc_timestamp
      from_unixtime
      from_utc_timestamp
      minute
      month
      months_between
      next_day
 
 video 99 -- spark -- Spark SQL -- Hive Functions -- Aggregate functions
          // the difference between these agg functions and what we have seen in spark is, here is take multiple record as input and one record as output. while in spark or scala agg function can take one record per group as input and one record per group as ouput.
       Below are few aggregate functions:
       count --> select count(*) from orders;
       sum --> select sum(order_item_subtotal) from order_items;
       avg --> 
       min -->
       max --> 
