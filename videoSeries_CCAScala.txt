All the details are at Itversity.com website under specific courses
       
Video 01 - Scala Fundamentals --> Have instructions on scala setup on mac with intellij IDE
Video 02 - Basic Scala Programming --> Var, val, loops
Video 03 - Functions --> definiing functions and anaonymous funcs.
Video 04 - OOPs concepts --> Class
     below command is used to set Java_Home environment path, so that we can use :javap command of scala.
        export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk
    Few Important points for Scala Class:
       Scala Class defination by default generate default constructor.
       If there is any argumnet of class with Val or Var, then those will be only argument of class and not variable of class.
       If arguments are defined by Val (immutable), getter method will be available by constructor.
       If arguments are defined by Var (mutable), getter and setter method will be available by constructor.
Video 05 - OOPs concepts --> Objects
       Objects are singleton class, hence it can not be intitable more than once. 
       There is no constructor for Objects.
       Companion Objects:- Class name and Object name are same.
Video 06 - OOPs concepts --> Case Classes
      This basically gives you many functionality as a inbuilt, without writting much code.
Video 07 - Scala Fundamentals --> Collection
      Sequence --> hold number of elements
          Array -->
          List -->
      Set --> unique values
      Map --> (Key,value) pair
      
MySql DB Details: (at time of certification, cloudera wouldn't allow to connect to mysql. we need to validate any mysql queries through sqoop)
       users --> 
              retail_user
              hr_user
              nyse_user
       available databases -->
              retail_db
              hr_db
              nyse_db
       hostname --> ms.itversity.com
       password --> itversity
       Commands --> 
              to connect to mysql --> mysql -u retail_user -h ms.itversity.com -p itversity
              to see available databases --> show databases;
              to connect to db --> use retail_db;
              to see tables --> show tables;
       
Sqoop commands:
       how to build connection string for mysql database.
              --> --connect jdbc:mysql://ms.iteversity.com:3306/retail_db
                  where, hostname --> ms.itevrsity.com
                         port --> 3306 (this is usually default port)
                         db_name --> retails_db
       Things to remember for --split-by option of sqoop command:
              1) Column should be indexed
              2) values should be sparse / or it should be sequentially generated or evenly incremented
              3) it should not have null values
              4) we need to use this always when we use --query aurgument
              
       --boundary-query aurgument is used for creating split while importing data.
       --table aurgument should not be used with --query aurgument.
       
Need to start from below video
video 44 -- apache sqoop -- sqoop import -- delimiters and handling nulls
Example Sqoop command:--
            sqoop import \
            --connect jdbc:mysql://ms.itversity.com:3306/hr_db \
            --username hr_user \
            --password itversity \
            --table employees \
            --warehouse-dir /user/dkothari/sqoop_import/hr_db \
            --null-non-string -1 \
            --fields-terminated-by "\t" \
            --lines-terminated-by ":"
            
video 47 -- Sqoop command for Hive import --
       sqoop import \
       --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
            --username retail_user \
            --password itversity \
            --table order_items \
            --hive-import \
            --hive-database dkothari_sqoop_import \
            --hive-table order_items \
            --num-mappers 2


sqoop import \
       --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
            --username retail_user \
            --password itversity \
            --table orders \
            --hive-import \
            --hive-database dkothari_sqoop_import \
            --hive-table orders \
            --hive-overwrite \
            --num-mappers 2
            
video 50 -->
       Question : Create Hive table "Daily_Revenue". based on the data fetched from other tables.
       Command: --> 
              create table daily_revenue as
              select order_date, sum(order_item_subtotal) daily_revenue
              from orders join order_items on 
              order_id = order_item_order_id
              where order_date like '2013-07%'
              group by order_date;
video 51 --> Sqoop export -- create hive table
       mysql command for creating table in retail_export --> create table daily_revenue_dkothari
              (order_date varchar(30), revenue float);
     
       sqoop export command-->
              sqoop export \
                     --connect jdbc:mysql://ms.itversity.com:3306/retail_export \
                     --username retail_user \
                     --password itversity \
                     --table daily_revenue_dkothari \
                     --export-dir /apps/hive/warehouse/dkothari_sqoop_import.db/daily_revenue \
                     --input-fields-terminated-by "\001"  
video 53 --> 
       mysql demo table --> 
              create table daily_revenue_dkothari_demo
              (revenue float, order_date varchar(30), description varchar(200));
     
      sqoop export command -->
              --column aurgument is very important. The order of column names in column aurgument should be same as source table                                 structure (which hdfs directory or hive table). Below is example:
              
              sqoop export \
                     --connect jdbc:mysql://ms.itversity.com:3306/retail_export \
                     --username retail_user \
                     --password itversity \
                     --table daily_revenue_dkothari_demo \
                     --export-dir /apps/hive/warehouse/dkothari_sqoop_import.db/daily_revenue \
                     --columns order_date,revenue \
                     --input-fields-terminated-by "\001" \
                     --num-mappers 1

video 54 --> sqoop update/ upsert/merge 
video 59 --> command to initiate the spark-shell
       spark-shell --master yarn --conf spark.ui.port=12654 (any 5 digit port no from 10000 to 65535)
      
       Some important spark-shell related aurguments:-->
      
      --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).
      
      Spark standalone and YARN only:
             --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,
                              or all available cores on the worker in standalone mode)
             --num-executors NUM         Number of executors to launch (Default: 2, in yarn mode).
                              
      hadoop / linux command to display size of data -->
              hadoop fs -du -s -h /user/dkothari
                     (here -s is to display size of files and -h is to show human readable format)
              
     spark-shell --master yarn \
       --conf spark.ui.port=12654 \
       --num-executors 1 \
       --executor-memory 512M
       
      All above information fro defaults are available at below listed location:
              under home directory (/user/dkothari)
                     $ view /etc/spark/conf/spark-env.sh
                     $ view /etc/spark/conf/spark-defaults.conf
                     
 command to stop currently running sparkContext -->
              sc.stop
 to initialize or start a new sparkContext now --> //initialize programmatically
       import.org.apache.spark.{sparkConf, sparkContext}
       val conf = new sparkConf().setAppName("Daily Revenue").master("yarn-client")
       val sc = new sparkContext(conf)
 
 Once the spark is up and running --> Below is the command to get the environment related details:
       sc.getConf.getAll.foreach(println)

video 60 --> Spark RDD (Resilient Distributed Dataset) -- lets rewatch it
       RDD --> extension of list dataset. It is in-memory and distributed dataset.
       
       //creating RDD -- validating files from file system
       hadoop fs -ls /user/dkothari/retail_db/orders
       
