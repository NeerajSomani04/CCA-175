All the details are at Itversity.com website under specific courses
       
Video 01 - Scala Fundamentals --> Have instructions on scala setup on mac with intellij IDE
Video 02 - Basic Scala Programming --> Var, val, loops
Video 03 - Functions --> definiing functions and anaonymous funcs.
Video 04 - OOPs concepts --> Class
     below command is used to set Java_Home environment path, so that we can use :javap command of scala.
        export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk
    Few Important points for Scala Class:
       Scala Class defination by default generate default constructor.
       If there is any argumnet of class with Val or Var, then those will be only argument of class and not variable of class.
       If arguments are defined by Val (immutable), getter method will be available by constructor.
       If arguments are defined by Var (mutable), getter and setter method will be available by constructor.
Video 05 - OOPs concepts --> Objects
       Objects are singleton class, hence it can not be intitable more than once. 
       There is no constructor for Objects.
       Companion Objects:- Class name and Object name are same.
Video 06 - OOPs concepts --> Case Classes
      This basically gives you many functionality as a inbuilt, without writting much code.
Video 07 - Scala Fundamentals --> Collection
      Sequence --> hold number of elements
          Array -->
          List -->
      Set --> unique values
      Map --> (Key,value) pair
      
MySql DB Details: (at time of certification, cloudera wouldn't allow to connect to mysql. we need to validate any mysql queries through sqoop)
       users --> 
              retail_user
              hr_user
              nyse_user
       available databases -->
              retail_db
              hr_db
              nyse_db
       hostname --> ms.itversity.com
       password --> itversity
       Commands --> 
              to connect to mysql --> mysql -u retail_user -h ms.itversity.com -p itversity
              to see available databases --> show databases;
              to connect to db --> use retail_db;
              to see tables --> show tables;
       
Sqoop commands:
       how to build connection string for mysql database.
              --> --connect jdbc:mysql://ms.iteversity.com:3306/retail_db
                  where, hostname --> ms.itevrsity.com
                         port --> 3306 (this is usually default port)
                         db_name --> retails_db
       Things to remember for --split-by option of sqoop command:
              1) Column should be indexed
              2) values should be sparse / or it should be sequentially generated or evenly incremented
              3) it should not have null values
              4) we need to use this always when we use --query aurgument
              
       --boundary-query aurgument is used for creating split while importing data.
       --table aurgument should not be used with --query aurgument.
       
Need to start from below video
video 44 -- apache sqoop -- sqoop import -- delimiters and handling nulls
Example Sqoop command:--
            sqoop import \
            --connect jdbc:mysql://ms.itversity.com:3306/hr_db \
            --username hr_user \
            --password itversity \
            --table employees \
            --warehouse-dir /user/dkothari/sqoop_import/hr_db \
            --null-non-string -1 \
            --fields-terminated-by "\t" \
            --lines-terminated-by ":"
            
video 47 -- Sqoop command for Hive import --
       sqoop import \
       --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
            --username retail_user \
            --password itversity \
            --table order_items \
            --hive-import \
            --hive-database dkothari_sqoop_import \
            --hive-table order_items \
            --num-mappers 2


sqoop import \
       --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
            --username retail_user \
            --password itversity \
            --table orders \
            --hive-import \
            --hive-database dkothari_sqoop_import \
            --hive-table orders \
            --hive-overwrite \
            --num-mappers 2
            
video 50 -->
       Question : Create Hive table "Daily_Revenue". based on the data fetched from other tables.
       Command: --> 
              create table daily_revenue as
              select order_date, sum(order_item_subtotal) daily_revenue
              from orders join order_items on 
              order_id = order_item_order_id
              where order_date like '2013-07%'
              group by order_date;
video 51 --> Sqoop export -- create hive table
       mysql command for creating table in retail_export --> create table daily_revenue_dkothari
              (order_date varchar(30), revenue float);
     
       sqoop export command-->
              sqoop export \
                     --connect jdbc:mysql://ms.itversity.com:3306/retail_export \
                     --username retail_user \
                     --password itversity \
                     --table daily_revenue_dkothari \
                     --export-dir /apps/hive/warehouse/dkothari_sqoop_import.db/daily_revenue \
                     --input-fields-terminated-by "\001"  
video 53 --> 
       mysql demo table --> 
              create table daily_revenue_dkothari_demo
              (revenue float, order_date varchar(30), description varchar(200));
     
      sqoop export command -->
              --column aurgument is very important. The order of column names in column aurgument should be same as source table                                 structure (which hdfs directory or hive table). Below is example:
              
              sqoop export \
                     --connect jdbc:mysql://ms.itversity.com:3306/retail_export \
                     --username retail_user \
                     --password itversity \
                     --table daily_revenue_dkothari_demo \
                     --export-dir /apps/hive/warehouse/dkothari_sqoop_import.db/daily_revenue \
                     --columns order_date,revenue \
                     --input-fields-terminated-by "\001" \
                     --num-mappers 1

video 54 --> sqoop update/ upsert/merge 
video 59 --> command to initiate the spark-shell
       spark-shell --master yarn --conf spark.ui.port=12654 (any 5 digit port no from 10000 to 65535)
      
       Some important spark-shell related aurguments:-->
      
      --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).
      
      Spark standalone and YARN only:
             --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,
                              or all available cores on the worker in standalone mode)
             --num-executors NUM         Number of executors to launch (Default: 2, in yarn mode).
                              
      hadoop / linux command to display size of data -->
              hadoop fs -du -s -h /user/dkothari
                     (here -s is to display size of files and -h is to show human readable format)
              
     spark-shell --master yarn \
       --conf spark.ui.port=12654 \
       --num-executors 1 \
       --executor-memory 512M
       
      All above information fro defaults are available at below listed location:
              under home directory (/user/dkothari)
                     $ view /etc/spark/conf/spark-env.sh
                     $ view /etc/spark/conf/spark-defaults.conf
                     
 command to stop currently running sparkContext -->
              sc.stop
 to initialize or start a new sparkContext now --> //initialize programmatically
       import.org.apache.spark.{sparkConf, sparkContext}
       val conf = new sparkConf().setAppName("Daily Revenue").master("yarn-client")
       val sc = new sparkContext(conf)
 
 Once the spark is up and running --> Below is the command to get the environment related details:
       sc.getConf.getAll.foreach(println)

video 60 --> Spark RDD (Resilient Distributed Dataset) -- lets rewatch it
       RDD --> extension of list dataset. It is in-memory and distributed dataset.
       
       //Validating files from HDFS file system
       hadoop fs -ls /user/dkothari/retail_db/orders
       
       //For simple dataset like list we use index to fetch the data at any specific location. for example, 
              val l = (1 to 100).toList ---> this is a command to create list in scala
              l(5) --> in this way we can fetch record at any specific location.
              
              Question:- 
                     how to get range of values in scala list. 
                     How to fetch last record in the list, if I don't know size. or How to get size of list.
       
      //Create RDD using spark-shell (for HDFS location)
       val orders = sc.textFile("/user/dkothari/retail_db/orders") --> scala command to create RDD and fetch data from text files which is                                                                        located at HDFS location.
       orders.first --> command to get first record from RDD
       orders.take(10) --> command to get top 10 records from RDD
       
        Question:-
              how to get range of values in scala RDD. 
              How to fetch last record in the RDD, if I don't know size. or How to get size of RDD.
       
       
                     
       //Read data from local file system
       val ProductsRaw = scala.io.Source.fromFile("/data/retail_db/products/part-00000").getLines.toList 
       
       --> above scala command is to read text file from local file system. "getLines" is the function to fetch each line from file and consider each line as one record. "toList" is function to convert it into scala list dataset.
        
        //Create RDD from local file system
        val productsRDD = sc.parallelize(ProductsRaw) --> parallelize function convert scala list (serialize) into RDD (distributed).

Video 61 --> DAG and Lazy Evaluation

DAG --> Directed Acyclic Graph
Lazy Evaluation --> spark will do lazy evaluation and create DAG till the program hit action. 

its not good practice to use foreach function on RDD directly. foreach functio should be used on scala collection like Array or list.

video 62 --> Reading different file format

       In spark, sparkContext doesn't have any functions to read data directly from different file formats. Hence, we need to use sqlContext API and sqlContext provide various functions to read data from different file format. Although, sqlContext also doesn't support avro file format.
       
       //command to read data from different file format
       sqlContext.read.json 
       sqlContext.read.parquet
       sqlContext.read.orc
       
       sqlContext.read and sqlContext.load --> both of these commands create spark Data Frame.
       
       // Difference between RDD and Dataframe is, RDD is just distributed dataset, while Dataframe is structured distribyted dataset.                     Dataframe is strctured more like a database table.
       val ordersDF = sqlContext.read.json("/user/dkothari/retail_db_json/orders")
       
       //command to preview the data
       ordersDF.show
       // Some Spark Dataframe APIs
       ordersDF.printSchema --> this will print dataframe columns names and datatype
       ordersDF.select("order_id", "order_date").show
       
       //below command is same as sqlContext.read.json
       sqlContext.load("/user/dkothari/reatil_db_json/orders", "json").show
       
video 64 --> spark transformations -- String manipulations
       Need to make sure about data type of original data. 
       Need to know how to type cast data. (toInt, toString, toDouble, etc are some APIs used to typecast)
       
       //below are few command to show how to do string manipulation
       //Create RDD using spark-shell (for HDFS location)
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val str = orders.first
       val A = str.split(',')
       val orderId = A(0).toInt
       val orderDate = A(1)
       val OrderCustID = A(2).toInt
       
       // Some other commands for string manipulation
       A(1).contains("2013")
       A(1).contains("2017")
       A(1).substring(5)
       A(1).substring(0, 9)
       A(1).replace('07','JUL')
       A(1).replace('-','/')
       A(1).indexof("2",2)
       A.length
       
       /Need to understand all these string manipulation function in more details

video 65 --> Spark -- Row level transformations (map function)
        // few frequently used API for this:
              map, flatMap, mapPartitions, mapPartitionsWithIndex
        
     Exercise:- Extract date from orders data (which is in "1,2013-07-25 00:00:00.0,11599,CLOSED" format) and convert "2013-07-25 00:00:00.0" date into 20130725 as Int format.
     Solution:- //commands below
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val orderDates = orders.map((str: String) => str.split(',')(1).substring(0,10).replace("-","").toInt)
       
       val ordersPairRDD = orders.map( (order: String) => {
                val o = order.split(',')
                (o(0).toInt, o(1).substring(0,10).replace("-","").toInt)
               })
              
        val orderItems = sc.textFile("/user/dkothari/retail_db/order_items")
        val orderItemsPairRDD = orderItems.map ( orderItem => {
              (orderItem.split(",")(1).toInt, orderItem)
              })

video 66 --> Spark -- Row level transformations (flatMap function)
       // Need to create a scenario to perform this. Hence, creating a list of word count problem.
       val l = List("Hello", "How are you doing", "let us perform word count", "As part of the word count problem", "we will see how many times each word repeat")
       val l_rdd = sc.parallelize(l)
       //Below command will split each record of RDD and then again split each record with " " (space char)) //
       val l_flatMap = l_rdd.flatMap( ele => ele.split(" ")) //this command run on each input item and split them into one or more
       val word_count = l_flatMap.map( word => (word,1)).countByKey
       
       //try to run below command to understand the diffenece between map and flatMap
       val l_map = l_rdd.map( ele => ele.split(" ")) //this command run on each input item and create each output
       
video 67 -- spark -- filter function
       // row level filtering:
       val orderFiltered = orders.filter(order => order.split(",")(3) == "COMPLETE")
       orderFiltered.count // this command is used to get the count of records in RDD
       
       Exercise:- Get all the orders of 2013-09 which are in closed or complete status
       Solution:- val orderFiltered = orders.filter(order => ((order.split(",")(3) == "COMPLETE" || order.split(",")(3) == "CLOSED") && 
                            order.split(",")(1).contains("2013-09"))
                            )
       orderFiltered.take(10).foreach(println)
       orderFiltered.count
       
       //below is the command to get distinct values of order status
       orders.map(order => order.split(",")(3).distinct
       
video 68 -- spark -- join function
       // join also support leftOuterJoin , rightOuterJoin, fullOuterJoin
       // joining orders and order_items data
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val orderItems = sc.textFile("/user/dkothari/retail_db/order_items")
       val ordersMap = orders.map(order => { 
              (order.split(",")(0).toInt, order.split(",")(1).substring(0,10))
              })
       val orderItemsMap = orderItems.map( orderItem => {
              (orderItem.split(",")(1).toInt, orderItem.split(",")(4).toFloat)
              })
       
       // take(10).foreach(println)
       val ordersJoin = ordersMap.join(orderItemsMap)

video 69 -- spark -- join // outerJoin
       Exercise: (problem statement) --> Get all the orders which do not have corresponding entries in order_items table
       Solution: 
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val orderItems = sc.textFile("/user/dkothari/retail_db/order_items")
       val ordersMap = orders.map(order => { 
              (order.split(",")(0).toInt, order)
              })
       val orderItemsMap = orderItems.map( orderItem => {
              (orderItem.split(",")(1).toInt, orderItem)
              })
       val ordersLeftOuterJoin = ordersMap.leftOuterJoin(orderItemsMap)
       val ordersLeftOuterJoinFilter = ordersLeftOuterJoin.filter( oi => oi._2._2 == None)
       val ordersWithNoOrderItems = ordersLeftOuterJoinFilter.map( o => o._2._1)
  
video 71 -- spark -- Aggregations -- using Actions APIs
      
       Exercise statment:- get the count of orders by order status 
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val ordersCountMap = orders.map( order => (order.split(",")(3), 1)).countByKey
       ordersCountMap.take(10).foreach(println)
       
       Exercise:- compute revenue for the month of 2013-09
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val orderItems = sc.textFile("/user/dkothari/retail_db/order_items")
       val orderItemsRevenue = orderItems.map( oi => oi.split(",")(4).toFloat)
       val orderItemsRevenueTotal = orderItemsRevenue.reduce((total, revenue) => total+revenue)
       
video 72 -- spark -- Aggregations -- combiner
      // differenece between groupByKey, reduceByKey, aggregateByKey APIs
      groupByKey --> it doesn't use combiner. Its performance is slower than reduceByKey and aggregateByKey
              1, (1 to 1000) -- sum(1 to 1000) => 1 + 2 + 3 + 4...1000
          
      reduceByKey --> it uses concept of combiner. which means aggregating based on combining result from intermediate values. But we need                              to use reduceByKey only when logic for all the intermediate computation and final compunatation is same. for                              example: (sum, count, etc). as you can see in syntext of reduceByKey, it uses only one func as aurgument to                                   compute the result.
              1, (1 to 1000) -- sum(1 to 1000) => (sum(1,250), sum(251,500), sum(501,750), sum(751,1000))
      aggregateByKey --> it is same as reduceByKey, the only difference is you can use it even if the computation logic for intermediate                                stage is different. for example (average, complex computation). As you can see in syntext of aggregateByKey, it uses                                two func as aurgument, one for intermediate computation and one for final combining computation to compute                                the final result. 
 
 video 73 -- spark -- groupByKey function
       Exercise statement:- get revenue by order_id 
       Solution:
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val orderItems = sc.textFile("/user/dkothari/retail_db/order_items")
       
       val ordersMap = orders.map(order => { 
              (order.split(",")(0).toInt, order)
              })
       val orderItemsMap = orderItems.map(orderItem => {
              (orderItem.split(",")(1).toInt, orderItem.split(",")(4).toFloat)
              })
       val orderItemsGBK = orderItemsMap.groupByKey
       val orderItemsRevenue = orderItemsGBK.map(rec => (rec._1,rec._2.toList.sum))
       
       Exercise statement:- Get data in descending order by order_item_subtotal for each order_id
       val ordersSortedByRevenue = orderItemsGBK.flatMap(rec => {
                                                       rec._2.toList.sortBy(o => -o).map(k => (rec._1, k))
                                                    })
                                                    
video 74 -- Aggregations -- reduceByKey
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val orderItems = sc.textFile("/user/dkothari/retail_db/order_items")
       val orderItemsMap = orderItems.map(orderItem => {
              (orderItem.split(",")(1).toInt, orderItem.split(",")(4).toFloat)
              })
       val RevenuePerOrderId = orderItemsMap.reduceByKey((total, revenue) => total+revenue)

video 75 -- Aggregations -- aggregateByKey
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val orderItems = sc.textFile("/user/dkothari/retail_db/order_items")
       val orderItemsMap = orderItems.map(orderItem => {
              (orderItem.split(",")(1).toInt, orderItem.split(",")(4).toFloat)
              })
       //input data format --> (order_id, order_item_subtotal)
       //output data format --> (order_id, (order_revenue, order_item_subtotal))
       
       val revenueAndMaxPerOrderID = orderItemsMap.aggregateByKey((0.0f,0.0f))(
       (inter, subtotal) => (inter._1+subtotal, if(subtotal > inter._2) subtotal else inter._2,
       (total, inter) => (total._1 + inter._1, if(total._2 > inter._2) total._2 else inter._2)
       )
       
      Homework Exercise Statement:- calculate revenueAndMinPerOrderId by looking at above solution. there will be some tweak and                                        challanges. its not same as revenueAndMaxPerOrderID calculation.

video 76 -- Spark core API -- SortByKey
       // Problem statement 1 -- Products Sorted by CategoryId
        val products = sc.textFile("/user/dkothari/retail_db/products")
        val productsMap = products.map(product => {
              (product.split(",")(1).toInt, product)
              })
        val productsSorted = productsMap.sortByKey()
        
         // Problem statement 2 -- Products Sorted by composite key 
                                   (ascending order by product_categoryId and descending order by product_price)
         val products = sc.textFile("/user/dkothari/retail_db/products")
         val productsMap = products.map(product => {
              ((product.split(",")(1).toInt, product.split(",")(4).toFloat), product)
              })
         
         While running the above Map command we found Error, (exception, NumberFormatException)
         To understand the issue as explained in video we need to do the data analysis.
         In video, below logic is one way to see the data and understand the reason for error:
         
         //so reason for error is, there is comma (,) in the 3rd data field itself. 
         val productFilterForError = products.filter(product => product.split(",")(4) == "")
         
         // So, we need to filter this record
         val productsMap = products.filter(product => product.split(",")(4) != "").
                            map(product => {
              ((product.split(",")(1).toInt, -product.split(",")(4).toFloat), product)
              })
         // In above statement if you see I neget the second part because its data type is float and we need product_price in decending order we can do it this way. but it cant be done with string data type. for string data type we need to use some other API logic like groupBy, etc.
         //Below is the command to get only the data field and not the key
         val productsSorted = productsMap.sortByKey().map(rec=> rec._2)
         productsSorted.take(10).foreach(println)


video 77 -- Spark -- API -- Ranking -- SortByKey and TakeOrdered
         //problem statement -- Get details of top 10 products by Price
         val products = sc.textFile("/user/dkothari/retail_db/products")
         val productsMap = products.
               filter(product => product.split(",")(4) != "").
               map(product => (product.split(",")(4).toFloat, product))
              
         val productsSorted = productsMap.sortByKey(false)
         
         //Below solution done useing TakeOrdered API
         val products = sc.textFile("/user/dkothari/retail_db/products")
         val productsMap = products.
               filter(product => product.split(",")(4) != "").
               takeOrdered(10)(Ordering[Float].reverse.on(product => product.split(",")(4).toFloat))
         val productsSortedByPrice = productsMap.foreach(println)
         
         Homework Problem statement:- Get details of top 10 products and get details of top 10 priced products. 
              //(Hint:- top 10 priced product may have more than 10 records)

video 78 -- spark -- core API - Ranking -- GroupByKey -- Products Per Category
              // Get top N priced product within each category
              val products = sc.textFile("/user/dkothari/retail_db/products")
              val productsMap = products.
                 filter(product => product.split(",")(4) != "").
                 map(product => (product.split(",")(1).toInt, product))
              val ProductsGrpByCategory = productsMap.groupByKey
              
video 79 -- spark -- core API -- Products per Category (continuation of above video) -- part 1 (getTopNPrices)
       // Here to understand it more accurately, we are dividing complete problem into multiple steps 
       //command to get first records collection inorder to understand data
              val productsIterable = ProductsGrpByCategory.first._2
              productsIterable.take(10).foreach(println) // this will show us all the products into that specific category
              productsIterable.size() // this will give us no of products (size of iterable object) into that category
       // By looking at result of above command, now we need to build logic to get top 5 products for that specific category
           
           //So, lets break our problem once again and get top 5 products prices first
               val productPrices =  productsIterable.map(p => p.split(",")(4).toFloat).toSet 
           // above command will retrive prices and we used set to remove duplicate value of prices
           // Now to get top 5 product prices from above result
              val topNProductPrices = productPrices.toList.sortBy(p => -p).take(5)
           //we converted set collection again to list because we want to use SortBy API. 
              
video 80 -- spark -- core API -- Products per Category (continuation of above video) -- part 2 (getTopNPricesProducts)          
           //Now, we need to get all the products in descending order by price
           val productsSortedByPrice = productsIterable.toList.sortBy(product => -product.split(",")(4).toFloat)
           val minOfTopNPrices = topNProductPrices.min
           val topNPricedProducts = productsSortedByPrice.takeWhile(product => product.split(",")(4).toFloat >= minOfTopNPrices)
           
           // why we didn't use filter API in above statement. because our data is already sorted so we don't need to go through all the records. takeWhile API will stop iterating once it will reach the specified condition.
           
           
           // Lets create a scala function to get top N product
           
           def getTopNProducts(productsIterable: Iterable[String], topN: Int): Iterable[String] = {
                val productPrices =  productsIterable.map(p => p.split(",")(4).toFloat).toSet 
                val topNProductPrices = productPrices.toList.sortBy(p => -p).take(topN)
                val productsSortedByPrice = productsIterable.toList.sortBy(product => -product.split(",")(4).toFloat)
                val minOfTopNPrices = topNProductPrices.min
                val topNPricedProducts = productsSortedByPrice.takeWhile(product => product.split(",")(4).toFloat >= minOfTopNPrices)
                topNPricedProducts
           }
   
video 81 -- spark -- core API -- Products per Category (continuation of above video) -- part 2 (getTopNPricesProductsByCategory)
       // Now we need to understand how we wants to get our output and based on that we need to decide which API we wants to use
       // spark map API will be used --> when you want to retrive RDD collection
       // spark flatMap API will be used --> when you want to retrive individual records 
