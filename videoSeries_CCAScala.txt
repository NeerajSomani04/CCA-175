All the details are at Itversity.com website under specific courses
       
Video 01 - Scala Fundamentals --> Have instructions on scala setup on mac with intellij IDE
Video 02 - Basic Scala Programming --> Var --> mutable, val --> immutable, loops (for loop, while loop, if-else, etc)
Video 03 - Functions --> defining functions and anaonymous funcs.
Video 04 - OOPs concepts --> Class
     below command is used to set Java_Home environment path, so that we can use :javap command of scala.
        export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk
    Few Important points for Scala Class:
       Scala Class definition by default generate default constructor.
       If there is any argumnet of class with Val or Var, then those will be only argument of class and not variable of class.
       If arguments are defined by Val (immutable), getter method will be available by constructor.
       If arguments are defined by Var (mutable), getter and setter method will be available by constructor.
Video 05 - OOPs concepts --> Objects
       Objects are singleton class, hence it can not be initiatable more than once. 
       There is no constructor for Objects.
       Companion Objects:- Class name and Object name are same.
Video 06 - OOPs concepts --> Case Classes
      This basically gives you many functionality as a inbuilt, without writting much code.
Video 07 - Scala Fundamentals --> Collection
      Sequence --> hold number of elements
          Array -->
          List -->
      Set --> unique values
      Map --> (Key,value) pair
      
MySql DB Details: (at time of certification, cloudera wouldn't allow to connect to mysql. we need to validate any mysql queries through sqoop)
       users --> 
              retail_user
              hr_user
              nyse_user
       available databases -->
              retail_db
              hr_db
              nyse_db
       hostname --> ms.itversity.com
       
       password --> itversity
       Commands --> 
              to connect to mysql --> mysql -u retail_user -h ms.itversity.com -p itversity
              to see available databases --> show databases;
              to connect to db --> use retail_db;
              to see tables --> show tables;
       
Sqoop commands:
       how to build connection string for mysql database.
              --> --connect jdbc:mysql://ms.iteversity.com:3306/retail_db
                  where, hostname --> ms.itevrsity.com
                         port --> 3306 (this is usually default port)
                         db_name --> retails_db
       Things to remember for --split-by option of sqoop command:
              1) Column should be indexed
              2) values should be sparse / or it should be sequentially generated or evenly incremented
              3) it should not have null values
              4) we need to use this always when we use --query aurgument
              
       --boundary-query aurgument is used for creating split while importing data.
       --table aurgument should not be used with --query aurgument.
       
video 33 -- sqoop -- listing databases and tables
       sqoop list-databases \
            --connect jdbc:mysql://ms.itversity.com:3306 \
            --username hr_user \
            --password itversity 
            
video 44 -- apache sqoop -- sqoop import -- delimiters and handling nulls
Example Sqoop command:--
            sqoop import \
            --connect jdbc:mysql://ms.itversity.com:3306/hr_db \
            --username hr_user \
            --password itversity \
            --table employees \
            --warehouse-dir /user/dkothari/sqoop_import/hr_db \
            --null-non-string -1 \
            --fields-terminated-by "\t" \
            --lines-terminated-by ":"
            
video 47 -- Sqoop command for Hive import --
       sqoop import \
       --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
            --username retail_user \
            --password itversity \
            --table order_items \
            --hive-import \
            --hive-database dkothari_sqoop_import \
            --hive-table order_items \
            --num-mappers 2


sqoop import \
     --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
     --username retail_user \
     --password itversity \
     --table orders \
     --hive-import \
     --hive-database dkothari_sqoop_import \
     --hive-table orders \
     --hive-overwrite \
     --num-mappers 2
            
video 50 -->
       Question : Create Hive table "Daily_Revenue". based on the data fetched from other tables.
       Command: --> 
              create table daily_revenue_test as
              select order_date, sum(order_item_subtotal) daily_revenue
              from orders o join order_items oi on 
              o.order_id = oi.order_item_order_id
              where order_date like '2013-07%'
              group by order_date;

       Notes:--> In above query where clause works fine in hive with Group By clause. But recommendation is to use having clause with Group By clause.

video 51 --> Sqoop export -- create hive table
       mysql command for creating table in retail_export --> 
              create table daily_revenue_dkothari
              (order_date varchar(30), revenue float);
     
       sqoop export command-->
              sqoop export \
                     --connect jdbc:mysql://ms.itversity.com:3306/retail_export \
                     --username retail_user \
                     --password itversity \
                     --table daily_revenue_dkothari \
                     --export-dir /apps/hive/warehouse/dkothari_sqoop_import.db/daily_revenue \
                     --input-fields-terminated-by "\001"  

Notes:--> Here, input field of hdfs file is terminated by ASCII char "\001". This is used to identify each field for export.

video 53 --> 
       mysql demo table --> 
              create table daily_revenue_dkothari_demo
              (revenue float, order_date varchar(30), description varchar(200));
     
      sqoop export command -->
              --column aurgument is very important. The order of column names in column aurgument should be same as source table                                 structure (which hdfs directory or hive table). Below is example:
              
              sqoop export \
                     --connect jdbc:mysql://ms.itversity.com:3306/retail_export \
                     --username retail_user \
                     --password itversity \
                     --table daily_revenue_dkothari_demo \
                     --export-dir /apps/hive/warehouse/dkothari_sqoop_import.db/daily_revenue \
                     --columns order_date, revenue \
                     --input-fields-terminated-by "\001" \
                     --num-mappers 1

video 54 --> sqoop update/ upsert/merge 
video 59 --> command to initiate the spark-shell
       spark-shell --master yarn --conf spark.ui.port=12654 (any 5 digit port no from 10000 to 65535)
      
       Some important spark-shell related aurguments:-->
      
      --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).
      
      Spark standalone and YARN only:
             --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,
                              or all available cores on the worker in standalone mode)
             --num-executors NUM         Number of executors to launch (Default: 2, in yarn mode).
                              
      hadoop / linux command to display size of data -->
              hadoop fs -du -s -h /user/dkothari
                     (here -s is to display size of files and -h is to show human readable format)
              
     spark-shell --master yarn \
       --conf spark.ui.port=12654 \
       --num-executors 1 \
       --executor-memory 512M
       
      All above information for defaults are available at below listed location:
              under home directory (/home/dkothari)
                     $ view /etc/spark/conf/spark-env.sh
                     $ view /etc/spark/conf/spark-defaults.conf
                     
 command to stop currently running sparkContext -->
              sc.stop
 to initialize or start a new sparkContext now --> //initialize programmatically
       import.org.apache.spark.{sparkConf, sparkContext}
       val conf = new sparkConf().setAppName("Daily Revenue").master("yarn-client")
       val sc = new sparkContext(conf)
 
 Once the spark is up and running --> Below is the command to get the environment related details:
       sc.getConf.getAll.foreach(println)

video 60 --> Spark RDD (Resilient Distributed Dataset) -- imp to understand RDD
       RDD --> extension of list dataset. It is in-memory and distributed dataset.
       
       //Validating files from HDFS file system
       hadoop fs -ls /user/dkothari/retail_db/orders
       
       //For simple dataset like list we use index to fetch the data at any specific location. for example, 
              val l = (1 to 100).toList ---> this is a command to create list in scala
              l(5) --> in this way we can fetch record at any specific location in list.
              
              Question:- 
                     how to get range of values in scala list. 
                     How to fetch last record in the list, if I don't know size. or How to get size of list.
       
      //Create RDD using spark-shell (for HDFS file location)
       val orders = sc.textFile("/user/dkothari/retail_db/orders") --> scala command to create RDD and fetch data from text files which is                                                                        located at HDFS location.
       orders.first --> command to get first record from RDD
       orders.take(10) --> command to get top 10 records from RDD
       
        Question:-
              how to get range of values in scala RDD. 
              How to fetch last record in the RDD, if I don't know size. or How to get size of RDD.
       
       
                     
       //Read data from local file system
       val productsRaw = scala.io.Source.fromFile("/data/retail_db/products/part-00000").getLines.toList 
       
       --> above scala command is to read text file from local file system. "getLines" is the function to fetch each line from file and consider each line as one record. "toList" is function to convert it into scala list dataset.
        
        //Create RDD from local file system
        val productsRDD = sc.parallelize(ProductsRaw) --> parallelize function convert scala serializable collections (ex. list, seq)  into RDD collection (distributed).

Video 61 --> DAG and Lazy Evaluation

DAG --> Directed Acyclic Graph
Lazy Evaluation --> spark will do lazy evaluation and create DAG till the program hit action. 

its not good practice to use foreach function on RDD directly. foreach function should be used on scala collection like Array or list.

video 62 --> Reading different file format

       In spark, sparkContext doesn't have any functions to read data directly from different file formats. Hence, we need to use sqlContext API and sqlContext provide various functions to read data from different file format. Although, sqlContext also doesn't support avro file format.
       
       //command to read data from different file format
       sqlContext.read.json 
       sqlContext.read.parquet
       sqlContext.read.orc
       
       sqlContext.read and sqlContext.load --> both of these commands create spark Data Frame.
       
       // Difference between RDD and Dataframe is, RDD is just distributed dataset, while Dataframe is structured distributed dataset.                     Dataframe is strctured more like a database table.
       val ordersDF = sqlContext.read.json("/user/dkothari/retail_db_json/orders")
       
       //command to preview the data
       ordersDF.show
       
       // Some Spark Dataframe APIs
       ordersDF.printSchema --> this will print dataframe columns names and datatype
       ordersDF.select("order_id", "order_date").show
       
       //below command is same as sqlContext.read.json
       sqlContext.load("/user/dkothari/reatil_db_json/orders", "json").show
       
video 64 --> spark transformations -- String manipulations
       Need to make sure about data type of original data. 
       Need to know how to type cast data. (toInt, toString, toDouble, etc are some APIs used to typecast)
       
       //below are few command to show how to do string manipulation
       //Create RDD using spark-shell (for HDFS location)
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val str = orders.first
       val A = str.split(',')
       val orderId = A(0).toInt
       val orderDate = A(1)
       val OrderCustID = A(2).toInt
       
       // Some other commands for string manipulation
       A(1).contains("2013")
       A(1).contains("2017")
       A(1).substring(5)
       A(1).substring(0, 9)
       A(1).replace('07','JUL')
       A(1).replace('-','/')
       A(1).indexof("2",2)
       A.length
       
       //Need to understand all these string manipulation function in more details

video 65 --> Spark -- Row level transformations (map function)
        // few frequently used API for this:
              map, flatMap, mapPartitions, mapPartitionsWithIndex
        
     Exercise:- Extract date from orders data (which is in "1,2013-07-25 00:00:00.0,11599,CLOSED" format) and convert "2013-07-25 00:00:00.0" date into 20130725 as Int format.
     Solution:- //commands below
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val orderDates = orders.map((str: String) => str.split(',')(1).substring(0,10).replace("-","").toInt)
       
       val ordersPairRDD = orders.map( (order: String) => {
                val o = order.split(',')
                (o(0).toInt, o(1).substring(0,10).replace("-","").toInt)
               })
              
        val orderItems = sc.textFile("/user/dkothari/retail_db/order_items")
        val orderItemsPairRDD = orderItems.map ( orderItem => {
              (orderItem.split(",")(1).toInt, orderItem)
              })

video 66 --> Spark -- Row level transformations (flatMap function)
       // Need to create a scenario to perform this. Hence, creating a list of word count problem.
       val l = List("Hello", "How are you doing", "let us perform word count", "As part of the word count problem", "we will see how many times each word repeat")
       val l_rdd = sc.parallelize(l)
       //Below command will split each record of RDD and then again split each record with " " (space char)) //
       val l_flatMap = l_rdd.flatMap( ele => ele.split(" ")) //this command run on each input item and split them into one or more
       val word_count = l_flatMap.map( word => (word,1)).countByKey
       
       //try to run below command to understand the diffenece between map and flatMap
       val l_map = l_rdd.map( ele => ele.split(" ")) //this command run on each input item and create each output
       
video 67 -- spark -- filter function
       // row level filtering:
       val orderFiltered = orders.filter(order => order.split(",")(3) == "COMPLETE")
       orderFiltered.count // this command is used to get the count of records in RDD
       
       Exercise:- Get all the orders of 2013-09 which are in closed or complete status
       Solution:- val orderFiltered = orders.filter(order => ((order.split(",")(3) == "COMPLETE" || order.split(",")(3) == "CLOSED") && 
                            order.split(",")(1).contains("2013-09"))
                            )
       orderFiltered.take(10).foreach(println)
       orderFiltered.count
       
       //below is the command to get distinct values of order status
       orders.map(order => order.split(",")(3)).distinct
       
video 68 -- spark -- join function
       // join also support leftOuterJoin , rightOuterJoin, fullOuterJoin
       // joining orders and order_items data
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val orderItems = sc.textFile("/user/dkothari/retail_db/order_items")
       val ordersMap = orders.map(order => { 
              (order.split(",")(0).toInt, order.split(",")(1).substring(0,10))
              })
       val orderItemsMap = orderItems.map( orderItem => {
              (orderItem.split(",")(1).toInt, orderItem.split(",")(4).toFloat)
              })
       
       // take(10).foreach(println)
       val ordersJoin = ordersMap.join(orderItemsMap)

video 69 -- spark -- join // outerJoin
       Exercise: (problem statement) --> Get all the orders which do not have corresponding entries in order_items table
       Solution: 
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val orderItems = sc.textFile("/user/dkothari/retail_db/order_items")
       val ordersMap = orders.map(order => { 
              (order.split(",")(0).toInt, order)
              })
       val orderItemsMap = orderItems.map( orderItem => {
              (orderItem.split(",")(1).toInt, orderItem)
              })
       val ordersLeftOuterJoin = ordersMap.leftOuterJoin(orderItemsMap)
       val ordersLeftOuterJoinFilter = ordersLeftOuterJoin.filter( oi => oi._2._2 == None)
       val ordersWithNoOrderItems = ordersLeftOuterJoinFilter.map( o => o._2._1)
  
video 71 -- spark -- Aggregations -- using Actions APIs
      
       Exercise statment:- get the count of orders by order status 
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val ordersCountMap = orders.map( order => (order.split(",")(3), 1)).countByKey
       ordersCountMap.take(10).foreach(println)
       
       Exercise:- compute revenue for the month of 2013-09
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val orderItems = sc.textFile("/user/dkothari/retail_db/order_items")
       val orderItemsRevenue = orderItems.map( oi => oi.split(",")(4).toFloat)
       val orderItemsRevenueTotal = orderItemsRevenue.reduce((total, revenue) => total+revenue)
       
video 72 -- spark -- Aggregations -- combiner
      // differenece between groupByKey, reduceByKey, aggregateByKey APIs
      groupByKey --> it doesn't use combiner. Its performance is slower than reduceByKey and aggregateByKey
              1, (1 to 1000) -- sum(1 to 1000) => 1 + 2 + 3 + 4...1000
          
      reduceByKey --> it uses concept of combiner. which means aggregating based on combining result from intermediate values. 
      But we need to use reduceByKey only when logic for all the intermediate computation and final compunatation is same. 
      for example: (sum, count, etc). as you can see in syntext of reduceByKey, it uses only one func as aurgument to compute the result.
              1, (1 to 1000) -- sum(1 to 1000) => (sum(1,250), sum(251,500), sum(501,750), sum(751,1000))
              
      aggregateByKey --> it is same as reduceByKey, the only difference is you can use it even if the computation logic for intermediate stage is different. for example (average, complex computation). As you can see in syntext of aggregateByKey, it uses two func as aurgument, one for intermediate computation and one for final combining computation to compute the final result. 
 
 video 73 -- spark -- groupByKey function
       Exercise statement:- get revenue by order_id 
       Solution:
       val orderItems = sc.textFile("/user/dkothari/retail_db/order_items")
       val orderItemsMap = orderItems.map(orderItem => {
              (orderItem.split(",")(1).toInt, orderItem.split(",")(4).toFloat)
              })
       val orderItemsGBK = orderItemsMap.groupByKey
       val orderItemsRevenue = orderItemsGBK.map(rec => (rec._1,rec._2.toList.sum))
       
       Exercise statement:- Get data in descending order by order_item_subtotal for each order_id
       val ordersSortedByRevenue = orderItemsGBK.flatMap(rec => {
                                                       rec._2.toList.sortBy(o => -o).map(k => (rec._1, k))
                                                    })
                                                    
video 74 -- Aggregations -- reduceByKey
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val orderItems = sc.textFile("/user/dkothari/retail_db/order_items")
       val orderItemsMap = orderItems.map(orderItem => {
              (orderItem.split(",")(1).toInt, orderItem.split(",")(4).toFloat)
              })
       val RevenuePerOrderId = orderItemsMap.reduceByKey((total, revenue) => total+revenue)

video 75 -- Aggregations -- aggregateByKey
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val orderItems = sc.textFile("/user/dkothari/retail_db/order_items")
       val orderItemsMap = orderItems.map(orderItem => {
              (orderItem.split(",")(1).toInt, orderItem.split(",")(4).toFloat)
              })
       //input data format --> (order_id, order_item_subtotal)
       //output data format --> (order_id, (order_revenue, order_item_subtotal))
       
       val revenueAndMaxPerOrderID = orderItemsMap.aggregateByKey((0.0f,0.0f))(
       (inter, subtotal) => (inter._1+subtotal, if(subtotal > inter._2) subtotal else inter._2,
       (total, inter) => (total._1 + inter._1, if(total._2 > inter._2) total._2 else inter._2)
       )
       
      Homework Exercise Statement:- calculate revenueAndMinPerOrderId by looking at above solution. there will be some tweak and                                        challanges. its not same as revenueAndMaxPerOrderID calculation.

video 76 -- Spark core API -- SortByKey
       // Problem statement 1 -- Products Sorted by CategoryId
        val products = sc.textFile("/user/dkothari/retail_db/products")
        val productsMap = products.map(product => {
              (product.split(",")(1).toInt, product)
              })
        val productsSorted = productsMap.sortByKey()
        
         // Problem statement 2 -- Products Sorted by composite key 
                                   (ascending order by product_categoryId and descending order by product_price)
         val products = sc.textFile("/user/dkothari/retail_db/products")
         val productsMap = products.map(product => {
              ((product.split(",")(1).toInt, product.split(",")(4).toFloat), product)
              })
         
         While running the above Map command we found Error, (exception, NumberFormatException)
         To understand the issue as explained in video we need to do the data analysis.
         In video, below logic is one way to see the data and understand the reason for error:
         
         //so reason for error is, there is comma (,) in the 3rd data field itself. 
         val productFilterForError = products.filter(product => product.split(",")(4) == "")
         
         // So, we need to filter this record
         val productsMap = products.filter(product => product.split(",")(4) != "").
                            map(product => {
              ((product.split(",")(1).toInt, -product.split(",")(4).toFloat), product)
              })
         // In above statement if you see I neget the second part because its data type is float and we need product_price in decending order we can do it this way. but it can't be done with string data type. for string data type we need to use some other API logic like groupBy, etc.
         //Below is the command to get only the data field and not the key
         val productsSorted = productsMap.sortByKey().map(rec=> rec._2)
         productsSorted.take(10).foreach(println)


video 77 -- Spark -- API -- Ranking -- SortByKey and TakeOrdered
         //problem statement -- Get details of top 10 products by Price
         val products = sc.textFile("/user/dkothari/retail_db/products")
         val productsMap = products.
               filter(product => product.split(",")(4) != "").
               map(product => (product.split(",")(4).toFloat, product))
              
         val productsSorted = productsMap.sortByKey(false)
         
         //Below solution done using TakeOrdered API
         val products = sc.textFile("/user/dkothari/retail_db/products")
         val productsMap = products.
               filter(product => product.split(",")(4) != "").
               takeOrdered(10)(Ordering[Float].reverse.on(product => product.split(",")(4).toFloat))
         val productsSortedByPrice = productsMap.foreach(println)

video 78 -- spark -- core API - Ranking -- GroupByKey -- Products Per Category
              // Get top N priced product within each category
              val products = sc.textFile("/user/dkothari/retail_db/products")
              val productsMap = products.
                 filter(product => product.split(",")(4) != "").
                 map(product => (product.split(",")(1).toInt, product))
              val ProductsGrpByCategory = productsMap.groupByKey
              
video 79 -- spark -- core API -- Products per Category (continuation of above video) -- part 1 (getTopNPrices)
       // Here to understand it more accurately, we are dividing complete problem into multiple steps 
       //command to get first records collection inorder to understand data
              val productsIterable = ProductsGrpByCategory.first._2
              productsIterable.take(10).foreach(println) // this will show us all the products into that specific category
              productsIterable.size() // this will give us no of products (size of iterable object) into that category
       // By looking at result of above command, now we need to build logic to get top 5 products for that specific category
           
           //So, lets break our problem once again and get top 5 products prices first
               val productPrices =  productsIterable.map(p => p.split(",")(4).toFloat).toSet 
           // above command will retrive prices and we used set to remove duplicate value of prices
           // Now to get top 5 product prices from above result
              val topNProductPrices = productPrices.toList.sortBy(p => -p).take(5)
           //we converted set collection again to list because we want to use SortBy API. 
              
video 80 -- spark -- core API -- Products per Category (continuation of above video) -- part 2 (getTopNPricesProducts)          
           //Now, we need to get all the products in descending order by price
           val productsSortedByPrice = productsIterable.toList.sortBy(product => -product.split(",")(4).toFloat)
           val minOfTopNPrices = topNProductPrices.min
           val topNPricedProducts = productsSortedByPrice.takeWhile(product => product.split(",")(4).toFloat >= minOfTopNPrices)
           
           // why we didn't use filter API in above statement. because our data is already sorted so we don't need to go through all the records. takeWhile API will stop iterating once it will reach the specified condition.
            
           // Lets create a scala function to get top N product
           
           def getTopNProducts(productsIterable: Iterable[String], topN: Int): Iterable[String] = {
                val productPrices =  productsIterable.map(p => p.split(",")(4).toFloat).toSet 
                val topNProductPrices = productPrices.toList.sortBy(p => -p).take(topN)
                val productsSortedByPrice = productsIterable.toList.sortBy(product => -product.split(",")(4).toFloat)
                val minOfTopNPrices = topNProductPrices.min
                val topNPricedProducts = productsSortedByPrice.takeWhile(product => product.split(",")(4).toFloat >= minOfTopNPrices)
                topNPricedProducts
           }
   
video 81 -- spark -- core API -- Products per Category (continuation of above video) -- part 3 (getTopNPricesProductsByCategory)
       // Now we need to understand how we wants to get our output and based on that we need to decide which API we wants to use
       // spark map API will be used --> when you want to retrive RDD collection
       // spark flatMap API will be used --> when you want to retrive individual records 

        val top3PricedProductsByCategory = ProductsGrpByCategory.flatMap( rec => (getTopNProducts(rec._2, 3))
        
video 82 -- spark -- core API -- setOperations
      sets --> are the collection of elements with similar data types and unique values
      set operations -->
              unique --> all unique values of all sets
              intersects --> common values within sets
              difference/minus --> for example, A - B = All elements of A that are not in B. 
      
      problem statement --> Get all customers who placed orders in Aug 2013 and Sep 2013
      
      val orders = sc.textFile("/user/dkothari/retail_db/orders")
      val customers_201308 = orders.
              filter(order => order.split(",")(1).contains("2013-08")).
              map(order => order.split(",")(2).toInt)
      
      customers_201308.distinct.count // by this command we can see how many unique custmers placed orders in 201308.
      
      val customers_201309 = orders.
              filter(order => order.split(",")(1).contains("2013-09")).
              map(order => order.split(",")(2).toInt)
      solution:
      
      val customers_201308_and_201309 = customers_201308.intersection(customers_201309)
      
      problem statement --> Get all unique customers who placed orders in Aug 2013 or Sep 2013
      Solution: 
      val customers_201308_union_201309 = customers_201308.union(customers_201309)
      customers_201308_union_201309.distinct.count
      
      problem statement --> Get all unique customers who placed orders in Aug 2013 but not in Sep 2013
      Solution:
      val customers_201308_minus_201309 = customers_201308.map( c => (c,1)).
              leftOuterJoin(customers_201309.map(c => (c,1))).
              filter( rec => rec._2._2 == None).
              map( rec => rec._1)
              
      customers_201308_minus_201309.distinct.count
      customers_201308_minus_201309.take(10).foreach(println)

video 83 -- spark -- core API -- save RDD in text File format and with delimiter
       // save RDD back to HDFS
       val orders = sc.textFile("/user/dkothari/retail_db/orders")
       val orderCountByStatus = orders.
              map( order => (order.split(",")(3), 1)).countByKey // this will outpout scala map but it doesn't have saveAsTextFile API
       
       val orderCountByStatus = orders.
              map(order => (order.split(",")(3), 1)).
              reduceByKey((total,ele) => total+ele) // reduceByKey is used so that we can use saveAsTextFile API.
       
       orderCountByStatus.saveAsTextFile("/user/dkothari/test_save_file/orders_count_by_status")
       hadoop fs -cat /user/dkothari/test_save_file/orders_count_by_status/part-00000 // command to view data
       
       orderCountByStatus.
              map(rec => rec._1 + "\t" + rec._2).
              saveAsTextFile("/user/dkothari/test_save_file_1/orders_count_by_status") 
              // this above is the same command as previous but with tab ("\t") as delimiter in the output file. We need to define delimilter only in the case of text file. for Json, parquet and other file we will see spark-sql (sqlContext).
       
       //Below is the command to preview the data
       sc.textFile("/user/dkothari/test_save_file_1/orders_count_by_status").take(10).foreach(println)
       
video 84 -- spark -- core API -- save RDD in textFile format, delimiter and compression
      orderCountByStatus.saveAsTextFile("/user/dkothari/test_save_file_compression/orders_count_by_status", classOf[org.apache.hadoop.io.compress.SnappyCodec])
      
      sc.textFile("/user/dkothari/test_save_file_compression/orders_count_by_status").take(10).foreach(println)    

video 85 -- spark -- core API -- save RDD data in different file format
       orc, json, parquet, avro
       Steps below:
              1) Data should be in spark Data Frame
              2) use write or Save API to save data frame in different file format
              3) we can also use compression algorithm if required
      
      val orderDF = sqlContext.read.json("/user/dkothari/retail_db_json/orders") //command to read json file and create data frame
      orderDF.write.orc("/user/dkothari/orders_orc") //command to write data into orc file format
      
      val orderDF = sqlContext.load("/user/dkothari/retail_db_json/orders", "json") //this command can also be use to read json file and create DF
      orderDF.save("/user/dkothari/orders_parquet", "parquet") //command to save data in parquet format
      
      sqlContext.load("/user/dkothari/orders_parquet", "parquet").show //command to view data after saving

video 86 -- spark -- Practice Problem statement -- understanding
       1) use retail_db
       2) problem statement 
              i) get daily revenue by product considering completed and closed orders
              ii) data needs to be sorted in ascending order by order_date and then descending order by revenue computed for each product for each day
              iii) data should be delimited by "," in this order --
                       order_date, daily_revenue_per_product, product_name
       3) data for orders and order_items are available in HDFS --
              /public/retail_db/orders and /public/retail_db/order_items
       4) data for products is available locally under --
              /data/retail_db/products
       5) Final output needs to be stored under --
              HDFS location - avro format:
              /user/dkothari/daily_revenue_avro_scala
              HDFS location - text format:
              /user/dkothari/daily_revenue_txt_scala
              Local location - text format:
              /home/dkothari/daily_revenue_txt_scala
              Solution needs to be stored under;
              /home/dkothari/daily_revenue_scala.txt
 
 video 87 -- spark -- Solution Launch spark shell
 
       // first lets see the size of data file
       hadoop fs -ls -h /public/retail_db/orders // 2.9 MB
       hadoop fs -ls -h /public/retail_db/order_items // 5.2 MB
       
       // now we need to understand the capacity of cluster, like number of executers, size of blocks, etc.
       //cloudera will provide resource manager web URL like itversity --> (rm01.itversity.com:8088)
       Couple of thing to understand and remember before we launch the spark-shell. We need to understand the resources available on cluster. Based on that we can decide and launch spark-shell with number of executors.
       i) No of resources/executor available
       ii) size of file that we are trying to process
       
      once you are on resourcemanager web URL: look for 2 important properties:
              1) Memory Total --> this is summation of multiplication of all Vcores and executor memory. [ sum( vcores * memory)]
              2) VCores Total --> this means no of processors or executers.
              3) executor memory allocation --> minimum and maximum allowable memory.
      
      1) Launch spark shell with proper configuration
      spark-shell --master yarn \
              --num-executors 12 \
              --executor-memory 2G \
              --conf spark.ui.port=12855 //this conf is only for itversity lab. for cloudera it might not be important.
      
video 88 -- spark -- Solution -- read, filter and join data    
      2) Read data: 
              val orders = sc.textFile("/public/retail_db/orders")
              val orderItems = sc.textFile("/public/retail_db/order_items")
           //its important to understand the data by previewing right away 
              orders.first
              orderItems.first
              orders.take(10).foreach(println)
              orderItems.take(10).foreach(println)
          
          // for exam day tip --> Durga sir mentioned that their will be different datasets and not only one like retail_db. Although, questions will be much simpler than what we are practicing in video series.

    3) Filter data for completed and closed orders (as per requirement)
          // when you apply any spark API, you need to make sure the datatype of RDD and its aurguments.
          // So, to understand data better and different values of order_status, we use below command
          orders.map(order => order.split(",")(3)).distinct.take(10).foreach(println)
          
       val ordersFiltered = orders.
              filter( order => order.split(",")(3) == "COMPLETE" || order.split(",")(3) == "CLOSED")
              
       ordersFiltered.take(10).foreach(pirntln) //command to view the data
       
      4) Convert both filtered orders and order_items into (key, value) pair, so that we can join both the dataset later
          
          val ordersMap = ordersFiltered.map( order => (order.split(",")(0).toInt, order.split(",")(1)))
          
          val orderItemsMap = orderItems.
              map(orderItem => (orderItem.split(",")(1).toInt, (orderItem.split(",")(2).toInt, orderItem.split(",")(4).toFloat)))
          
          ordersMap.first
          orderItemsMap.first
          
        5) join dataset to calculate revenue
              val ordersJoin = ordersMap.join(orderItemsMap)
              ordersJoin.first // to view data
            //output of join will look like this (order_id, (order_date, (order_item_product_id, order_item_subtotal)))
          
video 89 -- spark -- Solution -- calculate daily revenue per product
        6) calculate daily revenue per product_id
             //In order to calculate this we need to transform data into below format
             // ((order_date, order_item_product_id), order_item_subtotal)
             // ((order_date, order_item_product_id), daily_revenue_per_product_id) //here we did calculation
             //below is the code to get desired output here
             
             val ordersJoinMap = ordersJoin.
                                   map( rec => ((rec._2._1, rec._2._2._1), rec._2._2._2))
             val daily_revenue_per_product_id = ordersJoinMap.
                                                   reduceByKey((revenue, order_item_subtotal) => revenue + order_item_subtotal)
             daily_revenue_per_product_id.first // to view the data
             daily_revenue_per_product_id.count // to get the count of data items
             
video 90 -- spark -- Solution -- read and join products data that is available at local file system

         7) read products data from local file system and create RDD
              // sc.parallelize -- is used to create RDD from serialized scala collections (like, list, set, etc).
              // when we use scala.io API to read the file from local file system, we need to give fully qualified path of each file. It is not possible to read all the files under one specific directory by just giving directory name. 
              // file path should be given like this ("/data/retail_db/products/part-00000")
              // we have read each file seperately, update the collection and use sc.parallelize to create RDD.
              
              import scala.io.Source
              val productsRaw = Source.fromFile("/data/retail_db/products/part-00000").getLines
              
              //now you can see "productsRaw" object is of type Iterator[String]
              //Also, sc.parallelize() can not convert Iterator object into RDD. 
              //sc.parallelize() requires object of type sequence, (like List, Set, etc.)
              
              //below are the commands
               val productsRaw = Source.fromFile("/data/retail_db/products/part-00000").getLines.toList
               val products = sc.parallelize(productsRaw)
               
               //below are the commands to preview data
               products.first
               products.count
               products.take(10).foreach(println)
               
       8) join daily_revenue_per_product_id with products to get daily_revenue_per_product (by name)
               val productsMap = products.
                                     map( product => (product.split(",")(0).toInt, product.split(",")(2)))
               
               val daily_revenue_per_product_id_Map = daily_revenue_per_product_id.
                                     map( rec => (rec._1._2, (rec._1._1, rec._2)))
                                     
               daily_revenue_per_product_id_Map.first //to preview the data
               
               val dailyRevenuePerProductJoin = daily_revenue_per_product_id_Map.join(productsMap)
               
               dailyRevenuePerProductJoin.first //to view and verify data
               
 video 91 -- spark -- Solution -- Sort and save data is desired format
       9) sort the data in ascending order by order_date and daily_revenue_per_product in descending order
               // "dailyRevenuePerProductJoin"  this object is has data in below format
               //(product_id, ((order_date, daily_revenue), product_name))
               // but we need data in below format and it should be sorted properly
               //(order_date, daily_revenue, product_name)
               (24,((2013-07-25 00:00:00.0,319.96),Elevation Training Mask 2.0))
               val dailyRevenuePerProductJoinMap = dailyRevenuePerProductJoin.
                          map( rec => ((rec._2._1._1, -rec._2._1._2), (rec._2._1._1, rec._2._1._2, rec._2._2)))
               val dailyRevenuePerProductJoinMapSorted = dailyRevenuePerProductJoinMap.sortByKey()
               dailyRevenuePerProductJoinMapSorted.take(50).foreach(println)//to view data
               
       10) Get data in desired format -- order_date, daily_revenue, product_name
              // "mkString" is the scala function that can be applied to any collection, to convert all element of each record into one single string and you can also apply delimiter while forming your string. As you can see below code.
              // Below command will not work because it requires all the elements of the records in data type String. In our case, daily_revenue is of type Float. So, we need to do some changes in below code or use alternative approach.
              
              val dailyRevenuePerProduct = dailyRevenuePerProductJoinMapSorted.
                            map( rec => rec._2.mkString(","))
                            
               
               //As an alternative approach, we will use below code
                val dailyRevenuePerProduct = dailyRevenuePerProductJoinMapSorted.
                            map( rec => rec._2._1 + "," + rec._2._2 + "," + rec._2._3)
                
                dailyRevenuePerProduct.take(10).foreach(println)
                
      11) save data into HDFS location in text file format
                 dailyRevenuePerProduct.saveAsTextFile("/user/dkothari/daily_revenue_txt_file")
                 sc.textFile("/user/dkothari/daily_revenue_txt_file").take(10).foreach(println)
      12) save data into local file system in text file format   
            // below will copy all the files under specified directory
                 hadoop fs -get /user/dkothari/daily_revenue_txt_file /home/dkothari/daily_revenue_txt_scala
            //if the requirement is to copy entire directory then use below command
            //first we need to make sure the required directory is available at specific location, if not create it
               mkdir /home/dkothari/daily_revenue_scala_2
               hadoop fs -get /user/dkothari/daily_revenue_txt_file /home/dkothari/daily_revenue_scala_2/daily_revenue_txt_scala
                 
 video 92 -- spark -- Spark SQL -- Different Interface               
           // many tools available for spark SQL in different cluster environment.
            //Below is the command. this command will work in labs.itversity.com or HortonWorks. Not in Cloudera.
            
           1)  spark-shell --master yarn --conf spark.ui.port=12876
               then launch hive
            show databases; //command to view all available data bases.
            //In spark-sql query will compile into spark framework.
           
           2) // Hive is another tool
              hive --> just this command will lauch hive tool.
              //in the query will compile in map-reduce framework.
              
  video 93 -- spark -- Spark SQL -- Create Hive table and load data from and to Text File Format
          1) Create Database dkothari_retail_db_txt
          2) Create orders and Order_items table in dkothari_retail_db_txt database
          3) Load data into tables -- Data needs to be preformatted as per the table structure defined
          
          create database dkothari_retail_db_txt; 
          use dkothari_retail_db_txt;
          dfs -ls; // this command is equivalent to hadoop fs -ls in hive
          set hive.metastore.warehouse.dir; //this command is used to verify hive base dir path
          // hive.metastore.warehouse.dir=/apps/hive/warehouse
          
          dfs -ls /apps/hive/warehouse; // to list all available databases under hive.
          
           // In hive we don't use indexes or primary key. Its not reliable so far (01-01-2018).
           
           create table orders (
              order_id int,
              order_date string,
              order_customer_id int,
              order_status string) row format delimited fields terminated by ','
              stored as textfile;
              
          // textfile is default file format.
          show tables; //to list all available tables in db.
          select * from orders limit 10; //to sleect first 10 records from table.
             
          describe formatted orders; //this is a command to see definition of table
          
          // to load data from text file at local file system to hive table, below is the commmand
          load data local inpath '/data/retail_db/orders' into table orders;
          
          // to load data from text file at HDFS file system location to hive table, below is the commmand
          load data inpath '<hdfs file path location>' into table orders;
         
         // we need to be very careful with load command as it doesn't perform any cross validation.
         
          dfs -ls /apps/hive/warehouse/dkothari_retail_db_txt.db/orders;
          
          create table order_items (
              order_item_id int,
              order_item_order_id int,
              order_item_product_id int,
              order_item_quantity int,
              order_item_subtotal float,
              order_item_product_price float) row format delimited fields terminated by ','
              stored as textfile;
          
          load data local inpath '/data/retail_db/order_items' into table orders_items;

video 94 -- spark -- Spark SQL -- create database, tables and load data in ORC File Format
       
       create database dkothari_retail_db_orc; 
       use dkothari_retail_db_orc;
       
       create table orders (
              order_id int,
              order_date string,
              order_customer_id int,
              order_status string)
              stored as textfile;
              
       create table order_items (
              order_item_id int,
              order_item_order_id int,
              order_item_product_id int,
              order_item_quantity int,
              order_item_subtotal float,
              order_item_product_price float)
              stored as textfile;
              
       // now inorder to load data from local text file format to orc table in Hive, it will be 2 step process.
             // 1) Insert data in Hive table (call it as stage table) into the same format as of input text file format.
             // 2) copy data from this stage hive table to needed target orc table format.
       //below is one way of doing so
       
       insert into table dkothari_retail_db_orc.orders select * from dkothari_retail_db_txt.orders;
       insert into table dkothari_retail_db_orc.order_items select * from dkothari_retail_db_txt.order_items;
       
       select * from dkothari_retail_db_orc.orders limit 10;
       select * from dkothari_retail_db_orc.order_items limit 10;

video 95 -- spark -- Spark SQL -- Using spark-shell
       // we can write same queries that we used in Hive interface using spark-shell as well.
       // below are the syntext
       launch spark-shell --> spark-shell --master yarn --conf spark.ui.port=12876
       sqlContext.sql("any valid hive query")
       sqlContext.sql("select * from orders limit 10").show // show will print the result onto consol
 
video 96 -- spark -- Spark SQL -- Hive Functions
       show functions; // command to see some functions.
      // like  Aggregate functions (sum, count, min, max, etc)
      // String functions (substr/subString, instr, rlike, like, length, lcase or lower, ucase or upper, trim, ltrim, rtrim, lpad, rpad, cast, split etc)
      // date functions 
      
      describe function length; // this command will show syntext and defination of function
      select length('Hello World'); // this is the way to use the function.
      select length(order_status) from orders limit 10; // this is to use function on table
      select order_status, length(order_status) from orders limit 10;
      
video 97 -- spark -- Spark SQL -- Hive Functions -- manipulating strings
       // lets first create customer table in hive
       create table customers (
          customer_id int,
          customer_fname varchar(45),
          customer_lname varchar(45),
          customer_email varchar(45),
          customer_password varchar(45),
          customer_street varchar(45),
          customer_city varchar(45),
          customer_state varchar(45),
          customer_zipcode varchar(45)) row format delimited fields terminated by ','
          stored as textfile;
      
      load data local inpath '/data/retail_db/customers' into table customers;
      
      select substring('Hello World, How are you', 14);
      select substring('Hello World, How are you', 7, 5); // result is string 'World'
      select substr('Hello World, How are you', -3); //print last 3 char 'you'
      select substr('Hello World, How are you', -7, 3); // result will be 'are'
      select instr('Hello World, How are you', ' '); // 6 index position number
      select like('Hello World, How are you', 'Hello'); // this is a wrong syntext, below is correct one
      select 'Hello World, How are you' like 'Hello%'; // similar to data base commands
      select lower('Hello World');
      select lcase('Hello World');
      select initcap('hello world'); // this function will intialize every string will capital letter
      select trim(' hello world '); // it will trim left and right space charachter.
      select lpad(12, 3, '0'); // result will be 012
      select cast('12' as int); //it type cast any sting value
      select cast(substr(order_date, 6, 2) as int) from orders limit 10;
      select split('Hello World, how are you', ' '); // create array by spliting data
      select index(split('Hello World, how are you', ' '), 4); // access value of split data


video 98 -- spark -- Spark SQL -- Hive Functions -- manipulating Date 
      Below are few Hive Date functions:
      current_date --> select current_date;
      current_timestamp --> 
      date_add --> select date_add('2017-05-23', 1);
      date_format --> select date_format('2017-05-23', 'y');
      date_sub
      datediff
      day / dayofmonth
      to_date
      to_unix_timestamp
      to_utc_timestamp
      from_unixtime
      from_utc_timestamp
      minute
      month
      months_between
      next_day
 
video 99 -- spark -- Spark SQL -- Hive Functions -- Aggregate functions
          // the difference between these agg functions and what we have seen in spark is, here it take multiple record as input and one record as output. while in spark or scala agg function can take one record per group as input and one record per group as ouput.
       Below are few aggregate functions:
       count --> select count(*) from orders;
       sum --> select sum(order_item_subtotal) from order_items;
       avg --> 
       min -->
       max --> 

video 100 -- spark -- Spark SQL -- Hive Functions -- CASE / NVL
       CASE --> is a function of Hive, which is used for if-else condition statements in Hive SQL. as SQL doesn't support if-else statements.
       Lets understand this by an example:
       problem statement:- company wants to group similar kind of order_status from orders table. In order to create such groups we can use case ststements.
       solution:- 
       //lets first see different values of order_status.
       select distinct order_status from orders limit 10;
       //Below is the solution when you want to compare equal operators:
       
       select order_status, 
                    case order_status 
                       when 'COMPLETE' then 'No Action'
                       when 'CLOSED' then 'No Action'
                       when 'ON_HOLD' then 'Pending Action'
                       when 'PAYMENT_REVIEW' then 'Pending Action'
                       when 'PENDING' then 'Pending Action'
                       when 'PROCESSING' then 'Pending Action'
                       when 'PENDING_PAYMENT' then 'Pending Action'
                       else 'Risky'
                     end from orders limit 10;
          
           //Below is the solution when you want to compare non-equal operators:
                select order_status, 
                    case  
                       when order_status IN ('COMPLETE', 'CLOSED') then 'No Action'
                       when order_status IN ('ON_HOLD', 'PAYMENT_REVIEW', 'PENDING', 'PENDING_PAYMENT', 'PROCESSING') then 'Pending      Action'  
                       else 'Risky'
                     end from orders limit 10;
             
            // NVL function --> this function is used to handle "NULL" values in hive database.
            select nvl(order_status, 'Status Missing') from orders limit 10;
            // or we can write same thing with CASE
            select case when order_status is null then 'Status Missing' else order_status from orders limit 10;
            

video 101 -- spark -- Spark SQL -- Row level Transformations
      for example: Data Cleansing, Data Standarization
      
      //Below is one example:- 
      select concat(substr(order_date, 1, 4), substr(order_date, 6,2)) from orders limit 10;
      //convert above query result into int datatype
      select cast(concat(substr(order_date, 1, 4), substr(order_date, 6,2)) as int) from orders limit 10;
      //better way of writting same 
      select date_format(order_date, 'YYYYMM') from orders limit 10;
      select case(date_format(order_date, 'YYYYMM') as int) from orders limit 10;
 
video 102 -- spark -- Spark SQL -- Joining data from multiple tables

      //Below is old way of writting join statement
      select o.*, c.* from orders o, customers c 
      where o.order_customer_id = c.customer_id 
      limit 10;
      
      //Below is better and latest way of writting joins
      select o.*, c.* from orders o join customers c 
      on o.order_customer_id = c.customer_id 
      limit 10;
      
      //Below is query for left outer join
      select o.*, c.* from customers c left outer join orders o
      on o.order_customer_id = c.customer_id 
      limit 10;
      
      //Below is query for right outer join
      select o.*, c.* from orders o right outer join customers c
      on o.order_customer_id = c.customer_id 
      limit 10;
      
      //Another way of writting same queries // Perfomance is slow with below query
      select * from customers where customer_id not in (select distinct order_customer_id from orders) limit 10;
      
      //query to count the records
      select count(1) from orders o join customers c 
      on o.order_customer_id = c.customer_id;
      
      select count(1) from customers c left outer join orders o
      on o.order_customer_id = c.customer_id;

video 103 -- spark -- Spark SQL -- Aggregation in queries
      select order_status, count(1) as order_count from orders group by order_status;
      
      //Problem statement --> Get revenue for each order
      select o.order_id, o.order_date, sum(oi.order_item_subtotal) as order_revenue
      from orders o join order_items oi
      on o.order_id=oi.order_item_order_id
      group by o.order_id, o.order_date;
      
      //Problem statement --> Get revenue for each completed and closed orders and revenue greater than 1000.
      select o.order_id, o.order_date, sum(oi.order_item_subtotal) as order_revenue
      from orders o join order_items oi
      on o.order_id=oi.order_item_order_id
      where o.order_status in ('COMPLETE', 'CLOSED')
      group by o.order_id, o.order_date
      having sum(oi.order_item_subtotal)>=1000;
      
      //Problem statement --> Get revenue for each date
      select o.order_date, round(sum(oi.order_item_subtotal), 2) as daily_revenue
      from orders o join order_items oi
      on o.order_id=oi.order_item_order_id
      where o.order_status in ('COMPLETE', 'CLOSED')
      group by o.order_date;
      
video 104 -- spark -- Spark SQL -- Sorting in queries
     //Problem statement --> Get revenue for each order and sorted by order_date and then order_revenue
      select o.order_id, o.order_date,o.order_status, sum(oi.order_item_subtotal) as order_revenue
      from orders o join order_items oi
      on o.order_id=oi.order_item_order_id
      where o.order_status in ('COMPLETE', 'CLOSED')
      group by o.order_id, o.order_date, o.order_status
      having sum(oi.order_item_subtotal)>=1000
      order by o.order_date, order_revenue desc;
      
      //Problem statement --> on which day which order has generated highest revenue. Means highest revenue of each day.
      select o.order_id, o.order_date,o.order_status, sum(oi.order_item_subtotal) as order_revenue
      from orders o join order_items oi
      on o.order_id=oi.order_item_order_id
      where o.order_status in ('COMPLETE', 'CLOSED')
      group by o.order_id, o.order_date, o.order_status
      having sum(oi.order_item_subtotal)>=1000
      distribute by o.order_date sort by o.order_date, order_revenue desc;

video 105 -- spark -- Spark SQL -- Hive queries -- set Operations in queries
      // Difference between join and set Operations:
              join can be perform on any two tables or dataset by having a common key in them
              but set operations are performed only on similar kind of datasets. means unique and uniform structure.
              means same datatype and no duplicate values.
     
     // below is some example:
     //Below query will print result with all values, means Hello 2 times:
              select 1, "Hello"
              union all
              select 1, "World"
              union all
              select 2, "Hello"
              union all
              select 1, "world";
              
     //Below query will print result with all unique values, means Hello 1 time only:     
              select 1, "Hello"
              union 
              select 1, "World"
              union 
              select 2, "Hello"
              union 
              select 1, "world";
              
video 106 -- spark -- Spark SQL -- Hive Queries -- Analytics functions -- aggregation
 
         //Problem statement --> Get revenue for each order and how much percentage of each order toward revenue. Also, sorted by order_date and then order_revenue
         Solution:-
            select o.order_id, o.order_date, o.order_status, oi.order_item_subtotal, 
            round(sum(oi.order_item_subtotal) over (partition by o.order_id),2) as order_revenue,
            round(oi.order_item_subtotal/round(sum(oi.order_item_subtotal) over (partition by o.order_id), 2), 2) as pct_revenue
            from orders o join order_items oi
            on o.order_id=oi.order_item_order_id
            where o.order_status in ('COMPLETE', 'CLOSED')
            order by o.order_date, order_revenue desc;
            
 //Problem statement --> Get revenue for each order and  how much percentatge of each order toward revenue. Also, sorted by order_date and then order_revenue. Also, we need only those orders that has order_revenue greater than 1000.
 Solution:-
            select * from (select o.order_id, o.order_date, o.order_status, oi.order_item_subtotal, 
            round(sum(oi.order_item_subtotal) over (partition by o.order_id),2) as order_revenue,
            round(oi.order_item_subtotal/round(sum(oi.order_item_subtotal) over (partition by o.order_id), 2), 2) as pct_revenue
            from orders o join order_items oi
            on o.order_id=oi.order_item_order_id
            where o.order_status in ('COMPLETE', 'CLOSED')) q
            where order_revenue >= 1000
            order by order_date, order_revenue desc;
 
 video 107 -- spark -- Spark SQL -- Hive Queries -- Analytics functions -- ranking
        
        //Problem statement --> used multiple analytics functions. Learn definition of all functions from hive language documentation.
        Solution:-
            select * from (select o.order_id, o.order_date, o.order_status, oi.order_item_subtotal, 
            round(sum(oi.order_item_subtotal) over (partition by o.order_id),2) as order_revenue,
            round(oi.order_item_subtotal/round(sum(oi.order_item_subtotal) over (partition by o.order_id), 2), 2) as pct_revenue,
            round(avg(oi.order_item_subtotal) over (partition by o.order_id), 2) as avg_revenue,
            rank() over (partition by o.order_id order by oi.order_item_subtotal desc) as rnk_revenue,
            dense_rank() over (partition by o.order_id order by oi.order_item_subtotal desc) as dense_rnk_revenue,
            percent_rank() over (partition by o.order_id order by oi.order_item_subtotal desc) as pct_rnk_revenue,
            row_number() over (partition by o.order_id order by oi.order_item_subtotal desc) as rn_orderby_revenue,
            row_number() over (partition by o.order_id) as rn_revenue
            from orders o join order_items oi
            on o.order_id=oi.order_item_order_id
            where o.order_status in ('COMPLETE', 'CLOSED')) q
            where order_revenue >= 1000
            order by order_date, order_revenue desc, rnk_revenue;
            
            
 video 108 -- spark -- Spark SQL -- Hive Queries -- Windowing functions
  
  //Problem statement --> used multiple windowing functions. Learn defination of all functions from hive language documentation.
        Solution:-
            select * from (select o.order_id, o.order_date, o.order_status, oi.order_item_subtotal, 
            round(sum(oi.order_item_subtotal) over (partition by o.order_id),2) as order_revenue,
            round(oi.order_item_subtotal/round(sum(oi.order_item_subtotal) over (partition by o.order_id), 2), 2) as pct_revenue,
            round(avg(oi.order_item_subtotal) over (partition by o.order_id), 2) as avg_revenue,
            lead(oi.order_item_subtotal) over (partition by o.order_id order by oi.order_item_subtotal desc) as lead_ios,
            lag(oi.order_item_subtotal) over (partition by o.order_id order by oi.order_item_subtotal desc) as lag_ois,
            first_value(oi.order_item_subtotal) over (partition by o.order_id order by oi.order_item_subtotal desc) as fv_ois,
            last_value(oi.order_item_subtotal) over (partition by o.order_id order by oi.order_item_subtotal desc) as lv_ois 
            from orders o join order_items oi
            on o.order_id=oi.order_item_order_id
            where o.order_status in ('COMPLETE', 'CLOSED')) q
            where order_revenue >= 1000
            order by order_date, order_revenue desc;
            
            
  
video 109 -- spark -- Spark SQL -- Hive Queries -- Create Data frame and Register as temp table
       //Problem Statement - Spark SQL Application :-
       // Get daily revenue by product consdering Completed and Closed orders:
              1) Products needs to be read from local file system, Data Frame needs to be create
              2) Join Orders and Order_items
              3) Filter on Order_status
       // Data Needs to be sorted by ascending order by date and descending order by revenue compute for each order by each day
           1) Sort data by Order_date and in desc by computed revenue
      
      // Basic difference between RDD and Data Frame is, Data frame contains structure on top of data. while RDD is just simple formated text.
      
      Solution:-
            val ordersRDD = sc.textFile("/public/retail_db/orders")
            ordersRDD.take(10).foreach(println)
            
            val ordersDF = ordersRDD.map( order => {
                     (order.split(",")(0).toInt, order.split(",")(1),  order.split(",")(2).toInt, order.split(",")(3))
                     }).toDF
            ordersDF.show // it will show data frame but column names are like _1, _2, _3, _4
            
            //By below command you will get proper column name of data frame
            val ordersDF = ordersRDD.map( order => {
                     (order.split(",")(0).toInt, order.split(",")(1),  order.split(",")(2).toInt, order.split(",")(3))
                     }).toDF("order_id", "order_date", "order_customer_id", "order_status")
            ordersDF.show 
            ordersDF.printSchema
            ordersDF.registerTempTable("orders")
            sqlContext.sql("select * from orders").show
            
            val productsRaw = scala.io.Source.fromFile("/data/retail_db/products/part-00000").getLines.toList
            val productsRDD = sc.parallelize(productsRaw)
            val productsDF = productsRDD.map( product => {
                     (product.split(",")(0).toInt, product.split(",")(2))
                     }).toDF("product_id", "product_name")
            productsDF.show
            productsDF.registerTempTable("products")
            sqlContext.sql("select * from products").show
            
          
video 110 -- spark -- Spark SQL application --  Process Data
        // Spark SQL -- Appliation Solution:-
        1) Launh spark-shell
        spark-shell --master yarn --conf spark.ui.port=12564
        
        2) switch to the needed database
        sqlContext.sql("use dkothari_retail_db_txt")
        
        3) Need to read products data from local file system and create temp table for products
            val productsRaw = scala.io.Source.fromFile("/data/retail_db/products/part-00000").getLines.toList
            val productsRDD = sc.parallelize(productsRaw)
            val productsDF = productsRDD.map( product => {
                     (product.split(",")(0).toInt, product.split(",")(2))
                     }).toDF("product_id", "product_name")
            productsDF.show
            productsDF.registerTempTable("products")
        
        4) Need to configure spark sql jobs, as per cluster availability
        sqlContext.setConf("spark.sql.shuffle.partitions", "2")
        
        5) join all required tables 
        sqlContext.sql("SELECT o.order_date, p.product_name, sum(oi.order_item_subtotal) as daily_revenue_per_product " +
        "from orders o join order_items oi " +
        "on o.order_id=oi.order_item_order_id " +
        "join products p " +
        "on p.product_id=oi.order_item_product_id " +
        "where o.order_status in ('COMPLETE', 'CLOSED') " +
        "group by o.order_date, p.product_name " +
        "order by o.order_date, daily_revenue_per_product desc").show
        
        
 video 111 -- spark -- Spark SQL application --  Saving data in HDFS file format into HIVE
       Problem statment:- Use Hive and store the data to HIVE database dkothari_daily_revenue
              //get order_date, product_name, daily_revenue_per_product and save into hive table using orc file format
              
       Solution:-
              sqlContext.sql("create database dkothari_daily_revenue")
              
              sqlContext.sql("CREATE TABLE dkothari_daily_revenue.daily_revenue " +
              "(order_date string, product_name string, daily_revenue float) " +
              "STORED AS orc")
              
              //remember here we are running below query on "dkothari_retail_db_txt" database as we have all needed tables in that db.
              
              val daily_revenue_per_product = sqlContext.sql("SELECT o.order_date, p.product_name, sum(oi.order_item_subtotal) as daily_revenue_per_product " +
        "from orders o join order_items oi " +
        "on o.order_id=oi.order_item_order_id " +
        "join products p " +
        "on p.product_id=oi.order_item_product_id " +
        "where o.order_status in ('COMPLETE', 'CLOSED') " +
        "group by o.order_date, p.product_name " +
        "order by o.order_date, daily_revenue_per_product desc")
        
        daily_revenue_per_product.insertInto("dkothari_daily_revenue.daily_revenue")
        
        sqlContext.sql("select * from dkothari_daily_revenue.daily_revenue").show
       
       //Below are few other ways to write data to Hive table:
       //Below command is used when you don't have pre-existing table. //Need to explore, below command is not working
       1) daily_revenue_per_product.saveAsTable(tableName: "dkothari_daily_revenue.daily_revenue_1", mode: "orc")
       
       //Below command is used to save data into Hive in form of file system specification. //Need to explore, below command is not working
       2) daily_revenue_per_product.write.text
            
video 112 -- spark -- Spark SQL application -- DataFrame operations
       below are few:
              show
              filter
              select
              join
       
       daily_revenue_per_product.save("/user/dkothari/daily_revenue_json_hiveDF_Command", "json")
       daily_revenue_per_product.write.json("/user/dkothari/daily_revenue_json_hiveDF_Command_write")
       //incase you need to convert dataframe to RDD below is the command
       daily_revenue_per_product.rdd
       //below is the command just to select few columns from data frame
       daily_revenue_per_product.select("order_date").show
       daily_revenue_per_product.filter(daily_revenue_per_product("order_date") === "2013-07-25 00:00:00.0").show
       daily_revenue_per_product.filter(daily_revenue_per_product("order_date") === "2013-07-25 00:00:00.0").count
       //limit function creates a subset of original dataframe
       val daily_revenue_per_product_limit = daily_revenue_per_product.limit(10)

Data Ingestion:--

video 113 -- spark -- Streaming Analytics (Flume, Kafka, Spark-streaming)
       //Sqoop is used only for batch oprations (like, nightly once a day or something like that)
       // sqoop is used to transfer data from RDBMS to HDFS and vice-versa
       // While flume or kafka is used for Ingesting real time or near real-time streaming data into HDFS. like, server logs
       // process real-time streaming data as it is ingested onto cluster
       
video 114 -- spark -- Streaming Analytics -- Flume -- Documentation and example
       create a directory for our example:
       mkdir flume_demo
       //create a file in this directory
       vi flume_example //Below is the content of the file
       # example.conf: A single-node Flume configuration

       # Name the components on this agent
       a1.sources = r1
       a1.sinks = k1
       a1.channels = c1

       # Describe/configure the source
       a1.sources.r1.type = netcat
       a1.sources.r1.bind = localhost
       a1.sources.r1.port = 44444

       # Describe the sink
       a1.sinks.k1.type = logger

       # Use a channel which buffers events in memory
       a1.channels.c1.type = memory
       a1.channels.c1.capacity = 1000
       a1.channels.c1.transactionCapacity = 100

       # Bind the source and sink to the channel
       a1.sources.r1.channels = c1
       a1.sinks.k1.channel = c1
       
       //Below is the command to launch flume
       flume-ng agent --name a1 --conf-file /home/dkothari/flume_demo/example.conf
       
       //Below are the steps to explore flume more
       Launch another terminal window and connect to itversity lab
       //below is the command to communicate to flume agent
       telnet localhost 44444
       //Now whatever you are going to type here and hit enter, you can see that on the other terminal window where flume agent is running
       
video 115 -- spark -- Streaming Analytics -- Flume -- Get data from web server logs to HDFS
       source -- exec
       sink -- HDFS
       channel -- memory
       //In this video Only explanation of documentation and concepts 
       
video 116 -- spark -- Streaming Analytics -- Flume -- Setting up data
       //there is a good python tool available at cloudera QVM and labs for logs, "gen_logs" 
       location --> cd /opt/gen_logs
       //we are going to process logs that will be available by running below command
       tail -F /opt/gen_logs/logs/access.log
       
video 117 -- spark -- Streaming Analytics -- Flume -- Defining Source
        mkdir wslogstohdfs
        cp example.conf wslogstohdfs/
        cd wslogstohdfs/
        mv example.conf wshdfs.conf
        vi wshdfs.conf
        //by using below command, we are changing a1 agent name to wh as agent name. in the file wshdfs.conf
        :%s/a1/wh
        //by using below command, we are changing r1 to ws
        :%s/r1/ws
        //by using below command, we are changing c1 to mem
         :%s/c1/mem
        
       //So now the file "wshdfs.conf" content looks like below:
       #example.conf: A single-node Flume configuration

       # Name the components on this agent
       wh.sources = ws
       wh.sinks = k1
       wh.channels = mem

       # Describe/configure the source
       wh.sources.ws.type = exec
       wh.sources.ws.command = tail -F /opt/gen_logs/logs/access.log

       # Describe the sink
       wh.sinks.k1.type = logger

       # Use a channel which buffers events in memory
       wh.channels.mem.type = memory
       wh.channels.mem.capacity = 1000
       wh.channels.mem.transactionCapacity = 100

       # Bind the source and sink to the channel
       wh.sources.ws.channels = mem
       wh.sinks.k1.channel = mem
       
        
    //now we can start the flume agent
    flume-ng agent -n wh -f /home/dkothari/flume_demo/wslogstohdfs/wshdfs.conf
    //by running above command we are seeing data from logger
    

video 118 -- spark -- Streaming Analytics -- Flume -- Sink Introcustion
       //now lets change the sink name to hd
       :%s/k1/hd
       //add below lines in the sinks defination in conf file
       # Describe the sink
       wh.sinks.hd.type = hdfs
       wh.sinks.hd.hdfs.path = hdfs://nn01.itversity.com:8020/user/dkothari/flume_demo
       //the above hdfs path ca be found at this location : /etc/hadoop/conf/core-site.xml
       
       //now once we launch flume agent, it will be storing in Hdfs
       flume-ng agent -n wh -f /home/dkothari/flume_demo/wslogstohdfs/wshdfs.conf
       
       
       
