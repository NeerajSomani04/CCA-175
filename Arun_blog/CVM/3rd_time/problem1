1.

sqoop import \
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
--username retail_dba \
--password cloudera \
--table orders \
--warehouse-dir /user/cloudera/Arun_problems/3rd_time/problem1 \
--as-avrodatafile \
--compress \
--compression-codec=snappy

2.
sqoop import \
--connect jdbc:mysql://localhost:3306/retail_db \
--username retail_dba \
--password cloudera \
--table order_items \
--warehouse-dir /user/cloudera/Arun_problems/3rd_time/problem1 \
--as-avrodatafile \
--compress \
--compression-codec "org.apache.hadoop.io.compression.SnappyCodec"

3.
import com.databricks.spark.avro._
val ordersDF = sqlContext.read.avro("/user/cloudera/Arun_problems/3rd_time/problem1/orders")
val orderItemsDF = sqlContext.read.avro("/user/cloudera/Arun_problems/3rd_time/problem1/order_items")

4.a -- using DataFrame ---

val joinedDF = ordersDF.join(orderItemsDF, ordersDF("order_id") === orderItemsDF("order_item_order_id")).select(ordersDF("order_id"),ordersDF("order_date"),ordersDF("order_status"),orderItemsDF("order_item_subtotal"))

val DFResult = joinedDF.groupBy(to_date(from_unixtime(col("order_date")/1000)).alias("order_date"), col("order_status")).agg(countDistinct(col("order_id")).alias("total_orders"), round(sum(col("order_item_subtotal")),2).alias("total_amount")).orderBy(col("order_date").desc,col("order_status"),col("total_amount").desc,col("total_orders"))

4.b --- using spark SQL -----
val joinedDF = ordersDF.join(orderItemsDF, ordersDF("order_id")=orderItemsDF("order_item_order_id")).select(ordersDF("order_id"),ordersDF("order_date"),ordersDF("order_status"),orderItemsDF("order_item_subtotal"))

joinedDF.registerTempTable("joinedTemp")

val sparkSqlResult = sqlContext.sql("select to_date(from_unixtime(order_date/1000)) as order_date, order_status, count(distinct(order_id)) as total_orders, round(sum(order_item_subtotal),2) as total_amount from joinedTemp group by to_date(from_unixtime(order_date/1000)), order_status order by order_date desc, order_status, total_amount desc, total_orders")

4.c
val joinedRDD = joinedDF.map(x => ((x(1).toString,x(2).toString),(x(0).toString.toInt,x(3).toString.toFloat)))
val joinedGBK = joinedRDD.groupByKey()

val joinedGBKMap = joinedGBK.map{ case (key, value) => (key._1, key._2, value.toList)}


5.
sqlContext.setConf("spark.sql.parquet.compression.codec", "GzipCodec")
DFResult.save("/user/cloudera/Arun_problems/3rd_time/problem1/result4a-gzip", "parquet")

import org.hadoop.spark.sql.SaveMode
sparkSqlResult.write.format("parquet").mode(SaveMode.Overwrite).save("/user/cloudera/Arun_problems/3rd_time/problem1/result4b-gzip")

6.
sqlContext.setConf("spark.sql.parquet.compression.codec", "SnappyCodec")
DFResult.write.format("parquet").mode(SaveMode.Overwrite).save("/user/cloudera/Arun_problems/3rd_time/problem1/result4a-snappy")

sparkSqlResult.write.format("parquet").mode(SaveMode.Overwrite).save("/user/cloudera/Arun_problems/3rd_time/problem1/result4b-snappy")

7.
sqlContext.setConf("spark.sql.parquet.compression.codec", "uncompressed")
DFResult.map(x => x(0) + "," + x(1) + "," + x(2) + "," + x(3)).saveAsTextFile("/user/cloudera/Arun_problems/3rd_time/problem1/result4a-csv")
sparkSqlResult.map(x => x(0) + "," + x(1) + "," + x(2) + "," + x(3)).repartition(5).saveAsTextFile("/user/cloudera/Arun_problems/3rd_time/problem1/result4b-csv")

8.
mysql -h quickstart.cloudera -u retail_dba -p cloudera

mysql> use database retail_db;
mysql> create table result_problem1_3rdtime(order_date varchar(255), order_status varchar(255), total_orders int, total_amount decimal(10,2))

sqoop export \
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
--username retail_dba \
--password cloudera \
--table result_problem1_3rdtime \
--export-dir /user/cloudera/Arun_problems/3rd_time/problem1/result4a-csv 
