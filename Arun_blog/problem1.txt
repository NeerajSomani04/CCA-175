Every step solution:

sqoop import \
--connect jdbc:mysql://quickstart:3306/retail_db \
--username retail_dba \
--password cloudera \
--table orders \
--warehouse-dir /user/cloudera/Arun_problems/problem1 \
--compress \
--compression-codec org.apache.hadoop.io.compress.SnappyCodec \
--as-avrodatafile 


---------------------------------------------

sqoop import \
--connect jdbc:mysql://quickstart:3306/retail_db \
--username retail_dba \
--password cloudera \
--table order_items \
--warehouse-dir /user/cloudera/Arun_problems/problem1 \
--compress \
--compression-codec org.apache.hadoop.io.compress.SnappyCodec \
--as-avrodatafile 

-------------------------------------------
a. Problem solved using Data Frame API

import com.databricks.spark.avro._;

val orderDF = sqlContext.read.avro("/user/cloudera/Arun_problems/problem1/orders")
val orderItemsDF = sqlContext.read.avro("/user/cloudera/Arun_problems/problem1/order_items")
val joinedDF = orderDF.join(orderItemsDF, orderDF("order_id") === orderItemsDF("order_item_order_id"))

val DataFrameResult = joinedDF.groupBy(to_date(from_unixtime(col("order_date")/1000)).alias("order_date"), col("order_status")).agg(countDistinct(col("order_id")).alias("total_orders"), round(sum(col("order_item_subtotal")),2).alias("total_amount")).orderBy(col("order_date").desc, col("order_status"), col("total_orders").desc, col("total_amount"))

//if to_date is not supported by many file format
val DataFrameResult = joinedDF.groupBy(substring(from_unixtime(col("order_date")/1000), 1, 10).alias("order_date"), col("order_status")).agg(countDistinct(col("order_id")).alias("total_orders"), round(sum(col("order_item_subtotal")),2).alias("total_amount")).orderBy(col("order_date").desc, col("order_status"), col("total_orders").desc, col("total_amount"))

--------------------------------------------
b. Problem solved using Spark SQL 

import com.databricks.spark.avro._;

val orderDF = sqlContext.read.avro("/user/cloudera/Arun_problems/problem1/orders")
val orderItemsDF = sqlContext.read.avro("/user/cloudera/Arun_problems/problem1/order_items")
val joinedDF = orderDF.join(orderItemsDF, orderDF("order_id") === orderItemsDF("order_item_order_id"))

joinedDF.registerTempTable("orderAndOrderItems")

val sparkSqlResult = sqlContext.sql("select to_date(from_unixtime(cast(order_date/1000 as bigint))) as order_date, order_status, count(distinct(order_id)) as total_orders, cast(sum(order_item_subtotal) as decimal(10,2)) as total_amount from orderAndOrderItems group by to_date(from_unixtime(cast(order_date/1000 as bigint))), order_status order by order_date desc, order_status, total_amount desc, total_orders")

--------------------------------------------
c. Problem solved using spark RDD

---------------------------------------------
Store the result as parquet file into hdfs using gzip compression under folder

sqlContext.setConf("spark.sql.parquet.compression.codec","gzip")

DataFrameResult.write.parquet("/user/cloudera/Arun_problems/problem1/result4a-gzip")
sparkSqlResult.write.parquet("/user/cloudera/Arun_problems/problem1/result4b-gzip")

-----------------------------------------------
Store the result as parquet file into hdfs using snappy compression under folder

sqlContext.setConf("spark.sql.shuffle.partitions", "5") 
sqlContext.setConf("spark.sql.parquet.compression.codec","snappy")

DataFrameResult.write.parquet("/user/cloudera/Arun_problems/problem1/result4a-snappy")
sparkSqlResult.write.parquet("/user/cloudera/Arun_problems/problem1/result4b-snappy")

-----------------------------------------------
Store the result as CSV file into hdfs using No compression under folder
DataFrameResult.map(x => x(0) +"," + x(1) + "," + x(2) + "," + x(3)).saveAsTextFile("/user/cloudera/Arun_problems/problem1/result4a-csv")
sparkSqlResult.map(x => x(0) +"," + x(1) + "," + x(2) + "," + x(3)).saveAsTextFile("/user/cloudera/Arun_problems/problem1/result4b-csv")

------------------------------------------------

create a mysql table named result and load data from /user/cloudera/problem1/result4a-csv to mysql table named result 

mysql -u retail_dba -h quickstart -p cloudera

create table result_problem1 ( order_date DATETIME, order_status String(100), total_orders Int, total_amount decimal(10,2))

sqoop export \
--connect jdbc:mysql://quickstart:3306/retail_db \
--username retail_dba \
--password cloudera \
--table result_problem1 \
--export-dir "/user/cloudera/Arun_problems/problem1/result4a-csv"



