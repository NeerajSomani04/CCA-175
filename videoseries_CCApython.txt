video series link

https://www.youtube.com/watch?v=ScsRa6kyomY&index=16&list=PLf0swTFhTI8rT3ApjBqt338MCO0ZvReFt

video 01 Python Fundamentals - Introduction (for Spark)

python doesn't compile code but its an interprer based programming language. 
REPL --> READ, EVALUATE, PRINT and LOOP

exit() or Ctrl+D is used to come out of python prompt
ctrl+l --> to clear the screen

video 02 - Python basic programming constructs (decalring variables, invoking functions)
how to write function and for loop, while loop was discussed

03 Python Fundamentals - Functions and lambda functions

below is sample function that shows how can we yuse lambda function:

def sum(func, lb, ub):
  total = 0
  while(lb <= ub):
    total += func(lb)
    lb += 1
  return total

to use this function now we can run below commands with lambda or even with any other function:

sum(lambda i: i * i, 1, 10) --> this will print the sum of squares

04 Python Fundamentals - Collections (list, set, dictionary, tuple)

list --> mutable, can have duplicate values, also values are in ordered format
set --> mutable, no duplicate, order of values is not confirmed
tuple --> immutable, can have duplicate, ordered values
dictionary --> mutable, key-value pair format, key should be unique across dictionary

help(list) or help(set) , etc commands can be used to get the available list of APIs like append

list123.append('test') --> this command will append 'test' value in list123 list
list123.pop('test') --> is used to delete 'test' value
set123.add('test') --> this command will add 'test' value in set123 set

index, count, intersection, union, difference are few other important APIs that we should be aware of.

list = [1,2,3,4,5,6,7,8]
list[1:4] will print --> [2, 3, 4]
list[-1] will print --> [8]

dictionary --> key - value pair form
dict = {'1':'value1', '2':'value2'}

dict.keys() -->  this command will only return key of dictionary collection
dict.values() --> will return only values
dict.items() --> will return key-value pair in the form of tuple

has_key, get are few other API functions that can be used with dictionary

05 Python Fundamentals - Map, filter and Reduce APIs 
map --> this API is used to perform the trasformation on each element of each row
filter --> this API is used to perform any filteration of data set
reduce --> this API is used to perform the aggregation activity on dataset

below is one example for these API:
l = list(range(1,100)) --> this will create a list of element from 1 to 99

f = list(filter(lambda i: i%2 ==0, l)) --> using this f list will contain only even elements, means from 2 to 98 even numbers
m = list(map(lambda i: i * i, f)) --> will return list of square of each element
from functools import reduce
r = reduce(lambda total, element: total + element, m)

06 Scala or Python - Setup Data Set for basic I/O operations 
this video is to download dataset from github 

on lab all data-sets are available at path -- /data/retail_db

07 Python Fundamentals - IO Operations and processing data from files

to navigate to directory --> cd /data/retail_db/order_items

tail part-00000 ---> this command is used to see few records of file

below command to open the file and read records of the file in python shell -->

OrderItemsFile = open("/data/retail_db/order_items/part-00000", "r") --> this will return file object
OrderItemsRead = OrderItemsFile.read() --> this will return a str (string object)
OrderItems = OrderItemsRead.splitlines() --> this will return list object and we can perform all list operations
len(OrderItems] --> this will provide me number of records in file
OrderItems[-10:-1] --> this will return last 10 records from the list

Now we can perform any list operations on this. for example, map, reduce, filter

OrderItemsFilter = list(filter(lambda rec: int(rec.split(",")[1]) == 68880, OrderItems))
OrderItemsMap = list(map(lambda rec: float(rec.split(",")[4], OrderItemsFilter)
from functools import reduce
OrderItemRevenue = reduce(lambda total, element: total + element, OrderItemsMap)


08 Spark - Getting Started - HDFS Quick preview 
       properties files --> 
              /etc/hadoop/conf/core-site.xml
              /etc/hadoop/conf/hdfs-site.xml
      Important properties -->
              fs.defaultFS --> gives information about namenode URI (master deamon)
              dfs.BlockSize --> file partition based on this block size (example: 128 MB)
              dfs.replication --> environment will create copy of file to make it highly available (example: 3 replica)
      
      HDFS commands --> 
              copying file -->
                     --> from local file system to HDFS (hadoop fs -copyFromLocal or -put)
                     --> to local from HDFS (hadoop fs -copyToLocal or -get)
                     --> from one HDSF location to another (haddop fs -cp)
              listing file (hadoop fs -ls)
              previewing data from file (hadoop fs -tail or -cat)
              checking size of the file (hadoop fs -du -s -h filepath/filename)

09 Spark - Getting Started - YARN Quick Preview
YARN (Yet another resource nagotiator) 
       (distribution processing framework -- controls the execution of jobs)
       --> spark typically runs in YARN mode
       yarn configuration can be found at below location -->
              /etc/hadoop/conf/yarn-site.xml
              etc/spark/conf/spark-env.sh
       spark default settings --> these setting can help in performance
              Number of executor --> 2
              memory --> 1 GB

10 Spark - setup data set and data analysis

Below command will give you record count of each files in that specific folder.

wc -l /data/retail_db/*/*

video 01 Apache Spark Core APIs - Introduction

--> reading data from HDFS to RDD
--> then writing RDD data back to HDFS
--> Perform any needed operations on dataset

few examples of file formats --> JSON, AVRO, Parquet

problem statements --> 
1) Get daily revenue by product considering completed and closed orders. 
    Data needs to be sorted by ascending order by date and then descending order by revenue
    
video 03 Apache Spark Core APIs - Documentation
https://spark.apache.org/docs/1.6.3/programming-guide.html

05 Apache Spark Core APIs - Initializing the job using pyspark

pyspark --master yarn --conf spark.ui.port=12654

06 Apache Spark Core APIs - Create RDD from files using textFile

orders = sc.textFile("/user/dkothari/retail_db/orders")
orderItems = sc.textFile("/user/dkothari/retail_db/order_items")

to view the first record in RDD -->
orderItems.first()

to view top 10 records of RDD --> 
for i in orderItems.take(10): print(i)

07 Apache Spark Core APIs - Create RDD using parallelize function

read data from local file system:

ProductsRaw = open("/data/retail_db/products/part-00000").read().splitlines()

above command will give us list object

Below command will be used to create RDD from above created list:

ProductsRDD = sc.parallelize(ProductsRaw)
productsRDD.first() --> this command is used to fetch first record from RDD

08 Apache Spark Core APIs - Read data from different file formats

Data Frames -- (Distributed collection with structure)
API provided by SqlContext
supported file formats -- avro, json, parquet, orc
previewing the data --> show

 In spark, sparkContext doesn't have any functions to read data directly from different file formats. Hence, we need to use sqlContext API and sqlContext provide various functions to read data from different file format. 
       Although, sqlContext also doesn't support avro file format directly, but we can include some configuration inorder to read avro file using sqlContext. We will see this later.
       
       //command to read data from different file format
       sqlContext.read.json 
       sqlContext.read.parquet
       sqlContext.read.orc
       
       
       sqlContext.read and sqlContext.load --> both of these commands used to create spark Data Frame.
       
       // Difference between RDD and Dataframe is, RDD is just distributed dataset, while Dataframe is structured distributed dataset.                     Dataframe is strctured more like a database table.
              Hence, DataFrame = RDD + structured like table
       
       load and json are two ways to read data from hdfs and create Data frames
       
       ordersDF_read = sqlContext.read.json("/user/dkothari/retail_db_json/orders")
       ordersDF_read.show() --> this command can be used to preview the data
       
       ordersDF_load = sqlContext.load("/user/dkothari/retail_db_json/orders", "json")
       ordersDF_load.show() -->
       
 video 09 Apache Spark Core APIs - Row level transformations - String manipulation
 video 10 Apache Spark Core APIs - Row level transformations - using map
 
 again lets understand how this can be done in RDD
 
 orders = sc.textFile("/user/dkothari/retail_db/orders") --> this will create RDD
 
 below command can be used to perform some string operations on RDD
 for example, get the order_date from RDD and convert them into YYYYMMDD format
 
 OrdersMapDate = orders.map(lambda rec: int(rec.split(",")[1].split(" ")[0].replace("-","")))
 
 19 Spark SQL - Create Data Frame and Register Temp Table
 
 Orders = sc.textFile("/user/dkothari/retail_db/orders/part-00000")
 
 Below if one way to create dataframe out of RDD
 
 from pyspark.sql import Row
 OrdersDF = Orders.map(lambda rec: Row(order_id=int(rec.split(",")[0]), order_date=rec.split(",")[1])).toDF()
 
 Below if another way to create dataframe out of RDD
 
 OrdersDF = orders.map(lambda rec: Row(int(rec.split(",")[0]), rec.split(",")[1])).toDF(schema=["order_id", "order_date"])
 
 Below command is used to register the Data Frame as temp table:
 
 OrdersDF.registerTempTable("OrdersDF_table")
 
 Below command is used to view records from this table.
 sqlContext.sql("select * from OrdersDF_table").show(truncate=False)
 
 20 Spark SQL - Write Spark SQL Application for data processing
 
 Now we can use SQL queries to join tables and generate results
 Below command will create only 2 partition while processing the data
 sqlContext.setConf("spark.sql.shuffle.partitions","2")
 
 21 Spark SQL - Write Spark SQL Application - save output to Hive
 
 Once you have DB and table created in Hive as per needed specifications, then you can use various DF commands to write data to Hive tables.
 
 for example,:
 daily_revenue_per_products_df.insertInto("HiveDBName.HiveTableName")
 
 22 Spark SQL - Data Frame Operations
 
 
 
 
       
       
       


















