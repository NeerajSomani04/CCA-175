Scala Questions:
1) Create functions for Sum of Numbers, 
2) Sum of Square of num, 
3) cubes of nums, 
4) sum of range of multiples of 2s of numbers
5) how to get range of values in scala list. 
6) How to fetch last record in the list, if I don't know size. or How to get size of list.

Sqoop Questions:
-->  sqoop-import ------
1) Import data from mysql, compress it in snappy codec, and save file in both HDFS and Hive table in different file format.
2) Import data using sql query for same above sinario, with split-by, boundary-query clause.
3) Differnce between usage of warehouse-dir and target-dir aurgument.
4) How to handle null while importing or exporting.
5) usage of --fields-terminated-by "\t" and --lines-terminated-by ":" clause.
6) sqoop command for hive import.
-->  sqoop-export ------
1) practice few examples of sqoop-export scenarios.
2) 

Hive Questions:
1) Create new hive table by using available tables in hive or hdfs files.
2) 

Mysql question:
1) At time of exam how cloudera is going to provide details for mysql connection. 
Like mysql host address or how can we figure it out by ourself. 
I understand there would be definitely some details about user_name and password. 

HDFS questions:
1) How to find HDFS location of cluster.
cat /etc/hadoop/conf/core-site.xml
cat /opt/yarn/conf/yarn-site.xml
2) 

Spark Questions:
1) How to get basic configuration details of spark on cluster.
under local cluster home directory (/home/dkothari). (need to check local and hdfs dir for cloudera)
         $ view /etc/spark/conf/spark-env.sh
         $ view /etc/spark/conf/spark-defaults.conf
2) how to get range of values in scala RDD. 
3) How to fetch last record in the RDD, if I don't know size. or How to get size of RDD.
4) How to read data from local file system and text file has line delimiter as tab "\t" and field terminated as comma ",".
5) practice by creating RDD from both local and hdfs file system.
6) remember to prepare for avro file format. Both read and write. (SparkContext and SqlContext doesn't support Avro directly).

Some excercise questions from video:
video 66 --> Work on word count problem from scratch. means, creating data by usself and get the count of words.
video 67 --> Get all the orders of 2013-09 which are in closed or complete status
video 69 --> Get all the orders which do not have corresponding entries in order_items table
video 71 --> 
         1) get the count of orders by order status
         2) compute revenue for the month of 2013-09. and then compute monthly revenue.
video 73 --> (using GroupByKey API) Get revenue by order and sort the data in descending order by revenue. Final dataset should save at 
local file system directory in parquet format. Data should have columns -- (revenue, order_id) 
Homework Excercise: Learn how to sort string data type column in ascending or descending order. 
         For example, sort data in descending order by order_status.
video 74 --> same above excercise with reduceByKey.
video 75 --> (using AggregateByKey API) but output should be revenue, maxPerOrderID (max order_item_subtotal), order_id
//output data format --> (order_id, (order_revenue, order_item_subtotal)) saved in parquent format.
Homework Exercise Statement:- calculate revenueAndMinPerOrderId by looking at above solution. there will be some tweak and
challanges. its not same as revenueAndMaxPerOrderID calculation.
video 76 --> 
         1) Products Sorted by CategoryId
         2) Products Sorted by composite key 
                                   (ascending order by product_categoryId and descending order by product_price)
video 77 --> (SortByKey or TakeOrdered)
         1) Get details of top 10 products by Price
video 78, 79, 80, 81 --> Get top N priced product within each category. (try this with GroupByKey, reduceByKey and even with AggregateByKey)
         we need to get all the products in descending order by price. 
         Output should have same structure as products table data but just top 5 records by price for each category.
         
